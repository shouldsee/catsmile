
## 草稿

### 卷积生成式自编码器 CGAE

使用好处: 编码不一定是全局的. 对于LLGAE来说,只能表征一个全局图像附近的线性展开.无法分开表征两个
物体.也就是这个图片全局被认为是一个整体. CGAE放松了这个限制,通过把图片降解成几个组分的和,
并且每个组分具有横坐标纵坐标,允许图片中的物体自由地相对移动,但仍然不能表征物体之间的约束关系.
同时计算量也会增加许多. 但是只要允许的组分足够多,原型足够多,那么编码效果就应当渐进地好.

设计考量: 卷积不需要限制过多感受野,因为感受野可以靠原型内部的结构来加以限制. 线性编码是可选的,
会增加更多的计算量,好处是可以把类似的物品尽量合成进同一类物品,把每个类别扩展成低维流形,但是对于采样的要求
会更加高,因为每次Gibbs采样都需要多做几步梯度下降,对h的每个ijk组合找到最优的线性变量z.总的来讲,
计算量在 $O(IJR^2KHT)$ 其中IJ是图像的横坐标纵坐标,K是局部模型索引,H是组分数量,T是Gibbs周期数.当采样完成
的时候,可以直接用公式重建图片.

思考例子,有花纹,和没有花纹的衬衫. 图片如果有可分解的可加组分,比如空间上相对位置不固定的组分,就应当被
分解到K类别里.相对的,K类别里的原型应当是在空间上基本上总是协同出现的,反例较少的不可分割原子.



由于卷积对应着一个很自然的空间对称性,我们可以赋予生成器一个坐标体系,并允许生成器
用多个空间脉冲来表示一张图片.为防止符号冲突,这里用 $o$ 表示输入输出的图像,
每个组分涵盖坐标,和线性变量 $x({z_h}),y({z_h}),k(z_h),b(z_h)$ .求解这个优化问题
是highly non-trivial的,尤其是在多个组分 $k$ 上有求和式子的时候.

$$\begin{align}
\log q_m(o|z) &= -||o(x,y) - \sum_h \sum_{i,j,k} \delta(i-x(z_h),j-y(z_h)) f_m (x-i,y-j, k(z_h), b(z_h)) ||^2 \\
&= -||o(x,y) - \sum_h g_m(x({z_h}),y({z_h}),k(z_h),b(z_h)) ||^2 \\
 \hat z[o(x,y)] &= \text{argmax}_z \log q_m(o|z) 
\end{align}$$

对于这个无法直接优化的问题,我们选择在 $h$ 上交替进行优化/Gibbs采样.这意味着我们可能不需要
一个MAP点估计近似,而需要直接把 $\max$ 算子恢复成其连续形式,在求导数时,我们可以发现需要对z进行采样

$$\begin{align}
L(m) &= E_{p_{data}(x)}\left[\log \sum_z \exp[ -||o(x,y) - \sum_h g_m(x({z_h}),y({z_h}),k(z_h),b(z_h)) ||^2] \right] \\
L(m) &= E_{p_{data}(o)}[ \log \sum_z q_m(o|z) ] \\ 
{\partial  \over \partial m}  L 
&= E_{p_{data}(o)}[ \sum_z q_m(z|o) {\partial  \over \partial m}  q_m(o|z)] \\
&= E_{p_{data}(o),q_m(z|o)}[  {\partial  \over \partial m}  q_m(o|z)] \\
&= E_{p_{data}(o),q_m(z|o)}[  q_m(o|z){\partial  \over \partial m}  \log q_m(o|z)] \\
\end{align}$$

因此,对于 $q_m(z|o)$ 的采样,就决定了对于梯度的估算. 我们采用一个Gibbs采样器来对这个分布进行采样,在每一步,
我们对某一个组分进行采样,固定其余的组分不变.

实现伪代码:

```python
# 初始化模型
# 采集batch
self.K = 20 
self.GR = 20

K = self.K 
G = self.graph_dim
G2 = int(G)//0.5
GR = self.GR 
I = G2
J = G2
def _rand_sample(p):
  cp = p.cumsum(-1)
  _,idx = (torch.rand(p.shape) >= cp).max(-1)
  return idx

images = it.__next__()
x = images 
xw = self.xw.reshape((1,GR,GR,1,1,K)) # (1,GR,GR,1,1,K) 
I= G2,J=G2
dirac = torch.zeros( (1,1,1,I*J,1))
dirac = torch.scatter(dirac,index=(torch.arange(I)[:,None] * J 
    + torch.arange(J)[None,:]).reshape((1,1,1,I,J,1))
    .repeat((1,G2,G2,1,1))).rehsape((1,G2,G2,I,J,1))
wijk = torch.convolve2d(dirac,xw,dim=(1,2),method='same') #(1,G2,G2,I,J,K)

#初始化隐变量
#x = torch.zeros(shape,device=self.device)
#y = torch.zeros(shape,device=self.device)
#h = torch.zeros(shape,device=self.device)
zh = torch.zeros(shape,device=self.device).long()

beta = 1.
xrec = (x*0).detach()
res = xrec-x
for t in range(T):
  for h in range(H):
      #### 卷积产生
      wijk #(B,G,I,J,K)
      wijk #(B,G,I*J*K)
      #### 残差
      logp = -beta * (res - wijk).square().mean(1)
#      p = logp.softmax((-1,-2,-3)).reshape((B,I*J*K))
      idx = _rand_sample(logp.softmax(-1)) # (B,1)
      res = (res + wijk[:,:,zh] - wijk[:,:,idx])
      zh = torch.scatter(zh,index=h,value=idx,dim=1)



idx = zh
#idx_k = idx % K; idx = idx // K 
#idx_j = idx % J; idx = idx // J
#idx_i = idx % I; idx = idx // I
rec = torch.gather(wijk,index=zh[:,None,:],dim=2).sum(-1)
logp = - beta * (x-rec).square().mean(1)
loss =  logp.softmax(0).detach() * logp
loss = -loss.mean(0)
```

## 实践 CGAE:

实际按照在h上面做gibbs采样,速度实在太慢了.主要原因还是因为在空间上有卷积操作.
我这边考虑的比较核心的问题是alignment对齐问题. 在位置(i,j)放了一个物体k(比如
花瓶),这样的一个事实可以用图像域的加法来实现. 或者更稳定地,用一个替换来加以实现

$$
f(x) = x + g(i,j,k)
$$

考虑一个有序的解码过程,编码为 $z_1,z_2,z_3$

$$
f(z) = (0 - a(z_1, 0) + g_{ij}(z_1)) 
\\ - a[ z_2, (0 - a(z_1, 0) + g(z_1))] + g(z_2)
$$

这样的解码好处,是对于任意一个像素点,梯度只需要传导到最后进行覆盖的那个图片上,
而不需要考虑之前被覆盖的块. 这有点类似于graphics里面的alpha-blending.

一个比较类似的工作是2014基于RL训练CNN识别图片的方法, 通过对patch jump
 process进行RNN参数化后扔到分类损失函数里面求解.

如果我们考虑一个最为简单的损失函数,也即L2重建损失函数. 对重建器加以
限制为局域的拼图模型后,我们的目标就是用某个RNN去预测编码序列 $z$

$$\begin{align}
L(m) &= E_{p_{data}}[- D_{KL}(\delta(y=x|x)|| \sum_z q_e(z|x) q_r(x|z) )] \\
&= E_{p_{data}}[ \sum_y \delta(y|x) \log { \sum_z q_e(z|x) q_r(x|z) \over  \delta(y|x) } ] \\
&= E_{p_{data}}[  \log { \sum_z q_e(z|x) q_r(x|z)  } ] \\
&\geq E_{p_{data}}[  \sum_z { q_e(z|x)  \log q_r(x|z)  } ] \\
&= E_{p_{data},q_e(z|x)}[    \log q_r(x|z)   ] \\
\end{align}
$$

运用REINFORCE梯度估计的技巧

$$\begin{align}
{\partial \over \partial e} L(m) &= 
{\partial \over \partial e}  E_{p_{data}}[  \sum_z { q_e(z|x)  \log q_r(x|z)  } ] \\
&= 
 E_{p_{data}}[   \sum_z { q_e(z|x){\partial \over \partial e}  \log q_e(z|x)  \log q_r(x|z)  } ] \\
&= 
 E_{p_{data},q_e(z|x)}[   \log q_r(x|z) { {\partial \over \partial e}  \log q_e(z|x)   } ] 
\end{align}
$$

上面的式子味道有点怪,源自于对log函数使用了詹森不等式,如果不用凸函数下界直接求导数的话,可以发现需要用后验概率进行采样估计梯度,
因为解码器比较简单,也因此可以写成一个重要性抽样的形式

$$\begin{align}
{\partial \over \partial m} L(m) &= 
{\partial \over \partial m}   E_{p_{data}}[  \log { \sum_z q_e(z|x) q_r(x|z)  } ] \\
&=   E_{p_{data}}[ \sum_z { { \partial \over \partial m} q_e(z|x) q_r(x|z)  \over  \sum_z q_e(z|x) q_r(x|z) } ] \\
&=    E_{p_{data}}[ \sum_z { 
  q_e(z|x) q_r(x|z)   \over  \sum_z q_e(z|x) q_r(x|z) } { \partial \over \partial m} [\log q_e(z|x) + \log q_r(x|z) ] \\
&=   E_{p_{data}}[  {
  E_{q_e(z|x)} [q_r(x|z) { \partial \over \partial m} [\log q_e(z|x) + \log q_r(x|z) ] ]   \over  E_{q_e(z|x)}[ q_r(x|z) ]}  \\
\end{align}
$$

对于解码器,我们很简单地把每一个色块ijk涂抹到位置ij上具有局域结构k,可选线性偏差b,然后做一个序列上的last sum,就是最后一个涂抹色块的决定最后的值

编码器的目标就是要采样出这样的ijkb序列,让解码器可以恢复出原始的图像. 
我们让编码器具有一个RNN形式,每次单独预测di,dj,然后对于i,j位置的色块
进行k编码的采样,然后再预测di,dj. 我们让RNN可以接触到自己的所有历史,
在这个基础上再进行采样.

我们希望编码给出的是一个尽量稀疏的表示,不要给过多有关空白位置的信息,
从目标的角度看,编码可以用一个L2残差来进行指导. 如果我们对编码和解码器
进行一定的耦合,使得编码器可以用解码器来计算残差,那么我们只需要把残差输入
给到编码器,然后要求给出一个ijk组合即可.

但是这似乎需要构造一个数据集才能加以评估. 并且,仍然需要一个CNN来对输入
做并行处理. 引入RNN来指导采样,隐含了局域性的先验,也就是说RNN的运动被先
验地假设为局域的了. 这有助于研究RNN学习策略的能力,但要确保系统的高效性,
可能用CNN这种遍历算法还是更直观一点. 如果我们考虑一个简单的ktop编码器,
把CNN的输出在空间上通过离散采样稀疏化. 

那么在当下,还是简单地用一个求和模型来简单地测试这个框架的性能,这个求和模型很简单,
就是在 $C^{10}_3$也就是近720种组合里挑出最合适的编码来重建输入的图像.之所以之前的代码跑的很慢,很大一个原因就是在XY坐标上也做了组合优化.这里我们尝试继续采用
全局的构造. 也就是从残差图里,预测出一个组分.

但是头疼的是,这个模型用Gibbs采样时会卡在一个地方,就是使得图中央模糊一片的
这个全局平均.也就是发生了mode collapsing模式崩溃....目前并不知道具体是
啥原因

### Open Questions: 

为什么在RandomNNGAE里面会存在 Mode Collapsing的问题?  实际上是采样
的原因使得损失函数看起来很大. 我们发现SKLEARN的KMEANS和简单的用
argmax实现的KMEANS在K比较大的时候有一些差异,这可能是由于初始化方法
不同所造成的. 

```
hist = []
s = (i,j,h)
st = Wh @ h +  
```