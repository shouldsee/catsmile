#! https://zhuanlan.zhihu.com/p/529928772
# 1019: 机器学习中的变分方法与KL恒等式 Variational approaches in machine learning and KL equations
[CATSMILE-1019](http://catsmile.info/1019-vaml.html)

```{toctree}
---
maxdepth: 4
#caption: mycap
numbered: 0
---
1019-vaml.md
```


## 前言

- 目标: 举例澄清变分术语在机器学习中的相关应用. 
  梳理KL散度的恒等式子,方便建模
- 关键词: 
- 动机: 
  - 变分的词义跟VAE高度耦合,而跟梯度法耦合很差,
  影响术语的准确定义
- 应用:
  - 梯度下降
  - 变分生成模型
  - 变分推断
- 备注:
  - 变分法可谓是当代黑魔法了!
- CHANGELOG:
  - 20220801 修改公式对齐
  - 20220623 加入高斯混合模型的特例推导
  - 20220621 修复最大熵问题的积分错误

变分一词源自于物理数学中的变分法(Calculus of variations),最早可以追溯到最速降线问题的解决[见wiki](https://zh.wikipedia.org/wiki/%E5%8F%98%E5%88%86%E6%B3%95),后续在物理的各个分支中开枝散叶,在理论力学里体现为最小作用量原理,在热力学中也有体现. 变分法的目标一般是求某个泛函的极值. 在当代机器学习中,以概率散度为代表的泛函极值求解配合梯度优化广泛应用在了不同领域中,堪称机器学习的"第一性原理".下面我们来看一些例子

## 正文

### 特例一:最大熵问题

最简单的泛函就是一维曲线.考虑一个分布在(0,1)之间的概率密度函数$f(x)\geq 0$,一个简单的泛函就是其微分熵 $H(f)=\int -f(x)\log f(x)dx$,那么求解最大化这个泛函的问题,就是一个变分问题.我们知道,这个变分问题也叫最大熵问题,它的解就是均匀分布

$$\begin{align}
\hat f(x) &= \text{argmax}_f H(f) \\
\hat f(x) &= \mathbb 1(x \in [0,1] ) \\
H(\hat f) &= 0
\end{align}
$$

求解变分问题的一个办法是用一个参数化的函数族替换完全自由的函数空间, $\{f_m(x)\}$, 继而在这个参数空间上求解原始的优化问题,这个时候就可以利用各类参数优化方法了.比如我们可以对函数f进行重参数化,然后沿着梯度进行下降

$$\begin{align}
f_m(x) &= {\exp(mx)\over \int_0^1 \exp(mx) dx} \\
&= {\exp(mx) \over {1\over m }(\exp m -1)  }\\
&= {m\exp(mx) \over \exp m -1  }\\
f_{\hat m} &= \text{argmax}_m H(f_m)\\
&= \text{argmax}_m  \int -f(x)\log f(x)dx \\
{\partial \over \partial m} H(f_m) &= 
{\partial \over \partial m}  \int -f(x)\log f(x)dx
\\
&={\partial \over \partial m}  \int - {m\exp(mx) \over \exp m -1  } \log {m\exp(mx) \over \exp m -1  }dx \\ 
&={\partial \over \partial m}  \int  {m\exp(mx) \over \exp m -1  } \left( \log (\exp m -1) - \log  m\exp(mx) \right ) dx \\
&={\partial \over \partial m}  \left [ \log (\exp m -1) - \log  m +\int  {m\exp(mx) \over \exp m -1  } \left( -\log \exp(mx) \right ) dx \right ] \\
&= \left [ { \exp m \over \exp m -1} - {1\over m}  + {\partial \over \partial m} \int_0^1 - {\exp(mx) \over \exp m -1  } \left( mx \right ) d(mx) \right ] \\
&= \left [ { \exp m \over \exp m -1} - {1\over m}  - {\partial \over \partial m}  {[(mx - 1 )\exp (mx)]^1_0 \over \exp m -1  }  \right ] \\
&= \left [ { \exp m \over \exp m -1} - {1\over m}  - {\partial \over \partial m}  { (m-1)\exp(m) + 1 \over \exp m -1  }  \right ] \\
&= \left [ { \exp m \over \exp m -1} - {1\over m}  - {m\exp(m)\over \exp m -1 } +  { ((m-1)\exp(m) + 1)\exp m \over (\exp m -1 )^2 }  \right ] \\
&= \left [  - {1\over m}+{ -(m-1)\exp m \over \exp m -1}  +  { (m-1)\exp(m) \exp m \over (\exp m -1 )^2 }
+ { \exp m \over (\exp m -1 )^2 }
\right ] \\
&=\left [  \begin{aligned} &-{1\over m}+{ (m-1)\exp m (1-\exp m)\over (\exp m -1)^2}  \
+  { (m-1)\exp(m) \exp m \over (\exp m -1 )^2 }
\\&+ { \exp m \over (\exp m -1 )^2 }
\end{aligned} \right ]  \\
&= \left [  - {1\over m}+{ (m-1)\exp m \over (\exp m -1)^2}  
+ { \exp m \over (\exp m -1 )^2 }
\right ] \\
&= \left [  - {1\over m}+{ (m)\exp m \over (\exp m -1)^2}  
\right ] \\
\end{align}$$

然后搬出你的pytorch,jax,tf,或者任意梯度优化器,就可以验证能否解出 $m=0$ 了. 
虽然这个值看起来是发散的.但实际上在m=0附近趋近于0 [wolfram alpha](https://www.wolframalpha.com/input?i=%7Bx+exp%28x%29++%5Cover+%28exp+x+-+1%29%5E2+%7D+-+1%2Fx).(需要分析方面的大佬帮忙指点这个极限怎么手算?[TBC]) 事实上,对于带自动微分的优化器来说,也是可以用有限元方法去积分近似原始的目标函数的,这样就可以省去比较繁琐且容易出错的求导了.

### 特例二:最大交叉熵问题

交叉熵(cross-entropy)是神经网络中非常常用的一个损失函数,我们知道交叉熵和初始分布熵之差是KL散度,如果参数不影响初始熵 ${\partial \over \partial m }H(p)=0$,则最大化交叉熵和最小化KL散度是等价的. 

$$
\begin{align}
CE(p(x),q(x)) &= - \sum_x p(x) \log q(x)\\
&= \sum_x p(x) ( - \log q(x) + \log p(x) - \log p(x)) \\
&= \sum_x p(x) \log {p(x)\over q(x)} - \sum_x p(x) \log p(x) \\
&= -D_{KL}(p(x)||q(x)) + H(p) 
\end{align}
$$

像分类器的预测,就常常使用这种损失函数,给定输入 $x$ 和标签 $y$ 的条件分布 $p_{data}(y|x)$ 和神经网络函数族 $\{ q_m(y|x)\}$ 那么训练最优分类器通常可以写成如下形式. 任意判别器,比如ResNet,AlexNet,Vgg都是基于这个损失函数进行的扩展,在广义上也属于参数化了的变分问题.

$$
\begin{align}
\hat m &= \text{argmax}_m
CE(p_{data}(y|x),q_m(y|x)) \\
&= \text{argmax}_m
-D_{KL}(p_{data}(y|x)||q_m(y|x))
\end{align}
$$


注意任意概率散度理论上都可以成为用来训练分类器的损失函数,同时也会具有不同的性质.最大交叉熵使用了KL散度.从这个意义上讲,所有使用了KL散度的损失函数都应用了变分思想.

而事实上整个机器学习,只要运用了最优化的理论,某种程度上都是应用了变分思想,所以直接使用变分一词在机器学习里几乎是总是相关的,也因此是没有判别意义的.

### 特例三:变分推断 Variational Inference

从宏观上讲,VI的场景一般是生成模型形式已知 $p(x|z,m)$ ,
后验分布难以求解 $p(z|x,m)$ ,从某种程度上,
是一种逆向求解一个生成模型的技巧

变分推断来自Bayesian Posterior的推断问题,考虑的是
从可观测变量 $x$ (的分布)中推测隐变量 $z$ (的分布).
由模型w给出联合概率后,我们可以应用贝叶斯定理算出理论上 $z$ 的后验分布

$$
p(z|x,m) = {p(x,z|m)  \over \sum_z p(x,z|m) }
$$

但是实际上遍历隐变量求和的过程可能是难以计算的,于是可以考虑用函数族 $\{q_w(z|x)\}$ 来近似 $p(z|x,m)$,并且要求近似后验尽可能地接近真实后验,比如可以用KL散度衡量.通过最大化负KL散度,来得到一个接近真实后验的分布

$$
\begin{align}
&-D_{KL}(q(z|x) || p(z|x,m)) 
\\ &= \sum_z q_w(z|x) \log {p(z|x,m) \over  q_w(z|x)}
\\
&= \sum_z q_w(z|x) \log {p(z,x|m) \over  q_w(z|x) p(x|m)} \\
&= \sum_z q_w(z|x) \log {p(x|z,m)p(z|m) \over  q_w(z|x) p(x|m)} \\
&= \sum_z q_w(z|x) [\log {p(z|m)  \over  q_w(z|x) } + \log p(x|z,m) -\log p(x|m) \\
&= -D_{KL}(q_w(z|x)|| p(z|m) ) + \sum_z q_w(z|x) \log p(x|z,m) \\&\,\,\,\,\,\,\,- \sum_z q_w(z|x) \log p(x|m)] \\
&= -D_{KL}(q_w(z|x)|| p(z|m) ) +E_{q_w(z|x)} \log p(x|z,m) \\&\,\,\,\,\,\,\,- \log p(x|m) \\
\end{align}
$$

在推断的时候求解关于参数 $w$ 的极大值问题时,可以忽略不受w影响的项目,
如果假设均匀先验 $p(z|m)$,那么KL就简化为熵.注意对于后验分布的拟合,
自然地转化为了最大熵和似然期望的概念.

$$
\begin{align}
\hat w &= \text{argmax}_w [-D_{KL}(q(z|x) || p(z|x,m)) ]\\
 &= 
 \text{argmax}_w [-D_{KL}(q_w(z|x)|| p(z|m) ) +E_{q_w(z|x)} \log p(x|z,m) \\&\,\,\,\,\,\,\,- \log p(x|m)]  \\
 &= 
 \text{argmax}_w [-D_{KL}(q_w(z|x)|| p(z|m) ) +E_{q_w(z|x)} \log p(x|z,m) ] \\
 &= 
 \text{argmax}_w [ H(q_w(z|x)) +E_{q_w(z|x)} \log p(x|z,m) ] \\
\end{align}
$$

实际很多VI在写的时候会把 $q_w(z|x)$ 省略成 $q_w(z)$ ,这点在对比不同式子的时候需要注意.之所以能够省略,是因为在求解完成的时候有 $\hat w=f(x)$ ,但是在形式推理的时候直接省略是很让读者费解的

### 特例四: GMM

我们可以把GMM的损失函数看成是一个对于数据分布的直接逼近

$$\begin{align}
q(x|z) &= N(\mu_z,\Sigma_z) \\
q(x) &= \sum_z q(z) q(x|z) \\
L(m) &= D_{KL}(p_{data}(x) || q(x))\\
&= c+  E_{p_{data}(x)}[\log q(x)] \\
&= c+  E_{p_{data}(x)}[\log \sum_z q(z) q(x|z)]
\end{align}
$$

之所以可以这样做,是因为遍历z进行积分是可行的. 我们也可以加入一个编码器来得到
一个联合分布, 根据Gibbs不等式,这个函数在$q_e(z|x)$取到真实后验时最大化

$$\begin{align}
L(m) &= D_{KL}(p_{data}(x) q_e(z|x)|| q(z) q_r(x|z) )\\\
&= \sum_x \sum_z p_{data}(x) q_e(z|x) \log {q(z) q_r(x|z) \over   p_{data}(x) q_e(z|x) } \\
&= c + \sum_x p_{data}(x) \sum_z  q_e(z|x) \log {q(z) q_r(x|z) \over    q_e(z|x) } \\
&\leq c + \sum_x p_{data}(x) \sum_z  q_e(z|x)  \log \sum_z q(z) q_r(x|z)  \\
&= \sum_x p_{data}(x)   \log \sum_z q(z) q_r(x|z)  \\
\end{align}
$$

也就是说,对于生成分布是可以计算的模型来讲,不需要额外套用一个解码分布. 解码分布
一般是在生成分布的积分困难的时候,才加入,用来在隐变量上采样,近似计算局部的生成分布.


### 特例五:变分自编码器 VAE Variational Autoencoder

变分自编码器的KL损失很容易写岔,这里对比几两种视角,我们发现,区别在于概率分解的方法并不相同,第一种方法考虑的
是条件分布的拟合,在 $p_{data}(x)$ 上求期望,而第二种是联合分布的拟合.
我们熟知的VAE使用的是 $L_2$ 形式的损失函数

$$
\begin{align}
L_1 &=  -D_{KL}( p_{data}(x) \rightarrow \delta (y|x) ||
p_{data}(x) \rightarrow q_e(z|x) \rightarrow q_r(y|z) )\\
L_2 &= -D_{KL}( p_{data}(x) \rightarrow q_e(z|x) ||
w(z)  \rightarrow q_r(x|z)  ) \\
L_3 &= -D_{KL}(\sum_z p_{data}(x) \rightarrow q_e(z|x) ||
\sum_z w(z)  \rightarrow q_r(x|z)  )
\end{align}
$$

### 特例五: VAE-L1: 自编码条件概率KL的期望

考虑编码,解码过程,在概率上可以表示为条件分布和边际化

$$
x \rightarrow z|x \rightarrow y|z\\
$$

$$
\begin{align}
q(y|x) &=  \sum_z q(y,z|x) \\
&=\sum_z q(y|z) q(z|x) \\
&=\sum_z q_r(y|z) q_e(z|x)
\end{align}
$$

一个优秀的自编码器应当使得 $q(y|x)$ 概率流收缩到数据流形上的恒等映射, 
但是由于 $q(z|x)$ 有高斯噪音,所以不带降噪的恒等映射并不trivial.
这并不是一个trivial的目标. 注意 $q_r$ 中用了高斯扩散避免出现0概率.
条件概率的kl需要扩大到数据分布上,因此需要引入辅助变量 $y=x$.我们
先考察一个自然的想法,也就是拟合条件分布的KL散度的期望(见倒数第四行)

$$
\begin{align}
L_1 &= -D_{KL}(p_{data}(y,x)||q(y,x)) \\
&=  -D_{KL}( p_{data}(x)\delta (y|x) ||
p_{data}(x) q_e(z|x) q_r(y|z) )
\\
&= c + \sum_{x,y} p_{data}(y,x) \log q(y,x)\\
&= c + \sum_{x,y} p_{data}(x)\delta(y|x) \log p_{data}(x)q(y|x)\\
&= c + \sum_{x,y} p_{data}(x) \delta(y|x) (\log p_{data}(x) + \log q(y|x))\\
&= c + \sum_{x} p_{data}(x) \sum_y \delta(y|x) (\log p_{data}(x) + \log q(y|x))\\
&= c + \sum_{x} p_{data}(x) (\log p_{data}(x) + \sum_y \delta(y|x)  \log q(y|x))\\
&= c + \sum_{x} p_{data}(x) \sum_y \delta(y|x)  \log q(y|x)\\
&= c + E_{p_{data}(x)}[ \sum_y \delta(y|x)  \log q(y|x)]\\
&= c - E_{p_{data}(x)}[ D_{KL} ( \delta(y|x) ||  q(y|x))]\\
&= c + E_{p_{data}(x)}[ \log q(y=x|x)]\\
&= c + E_{p_{data}(x)}[ \log \sum_z q_r(y=x|z) q_e(z|x)]\\
&\geq  c + E_{p_{data}(x)}[ E_{q_e(z|x)} [ \log  q_r(y=x|z) ] ]
\end{align}
$$


这个原则上就是自编码所需要的损失函数了.但是实际上VAE的文章里还加了一个正则项,拉近 $q_e(z|x)$ 和一个先验分布 $q_w(z)$ 之间的距离,来提高采样/插值的效果.

### 特例五: VAE-L2:  联合概率之间的KL

这个损失函数是和常用的VAE损失等价的(待对比原文验证).这里接近了苏剑林的推导,将KL左右的数据分布和拟合分布分开分解的.左右两侧的分解形式并不一样.尤其在右边,没有出现编码器 $q_e(z|x)$, 而是直接用了生成模型的密度.让我们来看一下这种思路. 左侧分解成数据和编码器,右侧先生成编码,再解码成数据


$$
\begin{align}
&-D_{KL}(p_{data}(x,z)||q(x,z)) 
\\ &= -D_{KL}(p_{data}(x)q_e(z|x)||q_r(x|z)w(z)) 
\\
&= c + \sum_{x,z} p_{data}(x)q_e(z|x) \log {q_r(x|z)w(z)
\over p_{data}(x)q_e(z|x) } \\
&= c + \sum_{x} p_{data}(x) \sum_z q_e(z|x) \log {q_r(x|z)w(z)
\over q_e(z|x) } \\
&= c + \sum_{x} p_{data}(x) \sum_z q_e(z|x) \log q_r(x|z) + \sum_z q_e(z|x) \log {w(z) \over q_e(z|x) } \\
&= c + E_{p_{data}(x)} \left [ E_{q_e(z|x)}[ \log q_r(x|z)] - D_{KL}( q_e(z|x)  || w(z) )  \right ]\\
&= c + E_{p_{data}(x)} \left [ E_{q_e(z|x)}[ \log q_r(x|z)- \log q_e(z|x) + \log w(z) ]\right ]\\
\end{align}
$$

注意这里如果把KL分配进去期望里,求导的时候需要一点技巧,才能保留期望的形式,差了一个项其中第二项的梯度一定要求出 $\log q_e$ 才能计算. 这意味着我们
实际上并不需要闭式解,也可以通过直接求期望估算损失函数的导数. 这是因为参数进入了采样分布而造成的,我第一次见到这个技巧是在Policy Gradient策略梯度的推导里

$$\begin{align}
{\partial \over \partial m}L &= {\partial \over \partial m} E_{p_{data}(x)} [ - \sum_z q_e(z|x) \log q_e(z|x)]
\\
&= E_{p_{data}(x)} [ - \sum_z  ( {\partial \over \partial m} q_e(z|x)) \log q_e(z|x) +  q_e(z|x) {\partial \over \partial m} \log q_e(z|x)]
\\
&= E_{p_{data}(x)} [ - \sum_z  q_e(z|x)\left( {\partial \over \partial m} \log q_e(z|x)\right) \log q_e(z|x) 
\\ &\,\,\,\,+  q_e(z|x) {\partial \over \partial m} \log q_e(z|x)]
\\
&= E_{p_{data}(x)} [  E_ {q_e(z|x)}[ -\left( {\partial \over \partial m} \log q_e(z|x)\right) (1 +\log q_e(z|x))]
\\
\end{align}$$

### 特例五: VAE:  对比L2和L3

对比一下联合分布和边缘分布的区别,苏剑林的结果表明联合分布的负KL是边缘分布
的负KL的下界,我们来尝试推导一下

$$
\begin{align}
L_3 &= -D_{KL}(\sum_z p_{data}(x) q_e(z|x) ||
\sum_z w(z)  q_r(x|z)  ) \\
&=  \sum_x  \sum_z p_{data}(x) q_e(z|x) \left( \log
 \sum_z w(z)  q_r(x|z)  - \log \sum_z p_{data}(x) q_e(z|x) \right ) \\
&= c + \sum_x   p_{data}(x) \sum_z q_e(z|x) \left( \log
 \sum_z w(z)  q_r(x|z)  - \log \sum_z  q_e(z|x) \right ) \\
&= c + \sum_x   p_{data}(x) \sum_z q_e(z|x) \left( \log
 {\sum_z w(z)  q_r(x|z)  \over   \sum_z  q_e(z|x) }\right ) \\
&= c + \sum_x   p_{data}(x) \sum_z q_e(z|x) \left( \log
 {\sum_z w(z)  q_r(x|z)  \over   1 }\right ) \\
\end{align}
$$

直接一眼乍看上去不太能看出来差距有多少,尝试做差

$$
\begin{align}
& L_2 -L_3 
\\&= c + \sum_{x,z} p_{data}(x)q_e(z|x) \log {q_r(x|z)w(z)
\over p_{data}(x)q_e(z|x) } 
\\ &-  \sum_{x,z}   p_{data}(x)  q_e(z|x) \left( \log
 {\sum_z w(z)  q_r(x|z)  \over   p_{data}(x) \sum_z  q_e(z|x) }\right )\\
 &= c + \sum_{x,z} p_{data}(x)q_e(z|x) \left (\log {q_r(x|z)w(z)
\over p_{data}(x)q_e(z|x) } - \log {\sum_z w(z)  q_r(x|z)  \over   p_{data}(x) \sum_z  q_e(z|x) }\right )\\
 &= c + \sum_{x,z} p_{data}(x)q_e(z|x) \left (\log {q_r(x|z)w(z)
\over \sum_z w(z)  q_r(x|z)    } - \log {q_e(z|x) \over   \sum_z  q_e(z|x) }\right )\\
 &= c + \sum_{x} p_{data}(x) \sum_z q_e(z|x) \left (\log {q_r(x|z)w(z)
\over \sum_z w(z)  q_r(x|z)    } - \log {q_e(z|x) \over   1}\right )\\
&= c - \sum_{x} p_{data}(x) D_{KL}(q_e(z|x) || {q_r(x|z)w(z)
\over \sum_z w(z)  q_r(x|z) })\\
&\leq 0 \\
L_2 &\leq  L_3
\end{align}
$$

可以看到联合分布的负KL,严格小于边际分布的负KL,而且其中的差别在于左侧
编码分布 $q_e(z|x)$ 和右侧生成模型自然引导出的隐变量后验分布之间的KL散度.也就是说,对于一个完美的编码器,这两项取到等号

总结,VAE的损失可以理解为直接拟合一个联合分布的生成模型,这里有趣的点在于
构造联合分布的编码器和生成模型是同时拟合的.如果去掉编码器,那就意味着我们要
在z上求期望来拟合联合分布,而加入这个编码器意味着不用再求这个高维积分了,
并以一定的代价产生了一个下界. 所以说VAE和VI的联系紧密,而且是利用了VI的
思想直接拟合了一个生成模型 $w(z)q_r(x|z)$ ,极为有趣.

模型的构造,也就是左右概率的分解,直接影响我们对KL熵的计算,因此写下正确的拟合目标至关重要. 特别的,我们认为VAE是应用了VI技巧训练的生成模型,因此也可以考虑改称为 VIGM (Varitional Inference Generative Model)

$$
L_2 = -D_{KL}( p_{data}(x) q_e(z|x) || w(z) q_r(x|z))
$$

### KL恒等式: 散度定义

$$
- D_{KL} (p(x)||q(x)) = \sum_x p(x) (\log q(x) - \log p(x))\leq 0
$$

### KL恒等式:左右相同的增广变量

KL散度在两边用同一个条件概率增广后是不变的,(下面这个推导可以证明),所以上式事实上优化目标仍然是生成分布之间的散度

$$
\begin{align}
L &= -D_{KL}( p(x) q(y|x) || q(x) q(y|x)) \\
&= \sum_{x,y} p(x) q(y|x) [\log q(x) q(y|x) - \log p(x) q(y|x)] ) \\
&= \sum_{x} p(x)\sum_y q(y|x) [\log q(x)  - \log p(x) ) \\
&= \sum_{x} p(x) [\log q(x)  - \log p(x) ) \\
&\dots\\
&-D_{KL}( p(x) q(y|x) || q(x) q(y|x)) 
\\&= -D_{KL}(p(x)||q(x))
\end{align}$$


### KL恒等式:双向概率分解的联合概率和边缘概率的关系

结论是,联合分布的负KL是边缘分布负KL的下界

$$\begin{align}
& - D_{KL}(\sum_z p(x)e(z|x)|| \sum_z  w(z)g(x|z))
\\
&= \sum_{x,z} p(x)e(z|x) \log {\sum_z w(z)g(x|z) \over \sum_z p(x)e(z|x)}  \\
&= \sum_{x,z} p(x)e(z|x) \log {\sum_z w(z)g(x|z) \over p(x)}  \\
  &\,\,\,\,\,\,\,\,\,\,\,\cdot {  e(z|x) \over w(z)g(x|z)  } {   w(z)g(x|z)  \over e(z|x) }  \\
&= \sum_{x,z} p(x)e(z|x) \log  {  e(z|x) \over {w(z)g(x|z) \over \sum_z w(z)g(x|z)}  } {   w(z)g(x|z)  \over p(x)e(z|x) }  \\
&=\left( \begin{aligned}
  &\sum_{x} p(x) \sum_z e(z|x) \log  {  e(z|x) \over {w(z)g(x|z) \over \sum_z w(z)g(x|z)}  }\\
  &+ \sum_{x,z} p(x)  e(z|x){   w(z)g(x|z)  \over p(x)e(z|x) } 
  \end{aligned} \right)  
  \\
  &= \left( \begin{aligned}
  &\sum_{x} p(x) D_{KL}( e(z|x) || {w(z)g(x|z) \over \sum_z w(z)g(x|z)}  ) \\
  &- D_{KL}(p(x)  e(z|x) ||  w(z)g(x|z) )
    \end{aligned} \right)  
    \\
   & \dots\\
&- D_{KL}(\sum_z p(x)e(z|x)|| \sum_z  w(z)g(x|z)) &\\ 
&\geq
- D_{KL}(p(x)  e(z|x) ||  w(z)g(x|z) )\\
\end{align}
$$

### MAP不等式: logsumexp

边际化中经常会用到配分函数的对数. 这个配分函数可以用最大后验点来作为近似
下界

$$\begin{align}
p(x) &= \log \sum_y \exp ( \log p(x,y))\\
    &\geq  \max_y  \log p(x,y)
\end{align}$$


## 参考

苏剑林变分推断新解 <https://zhuanlan.zhihu.com/p/40105143>或<https://kexue.fm/archives/5716>


## 草稿




如果强行尝试应用苏剑林提到的有关增广变量的KL不等式,如果x和z的联合分布可以被
很好地拟合,那么x本身的分布也可以被很好拟合.而我的推导表明,加入了增广变量以后,不能自动转化成一个隐变量上的KL散度. 特别地,如果用同一个条件概率
去增广两个不同的分布,增广前后的KL散度是不变的,因为增广变量没有引入新的信息.区别在于,



$$
-D_{KL}(p_{data}(y,x) g(z|x,y)||q(y,x) g(z|x,y))  
\\=
-D_{KL}(p_{data}(y,x) ||q(y,x) )  
$$

$$\begin{align}
g(z|x,y) &= {h(x,z,y)\over \sum_z h(x,z,y)}\\
&= {  [ q(z)q(z|x) ] q(y|z)\over \sum_z [ q(z)q(z|x) ] q(y|z)}
\end{align}$$

$$\begin{align}
L &=
c -D_{KL}(\,p_{data}(y,x) \cdot g(z|x,y) \,||
\, q(y,x) \cdot g(z|x,y)\,)  \\
&=  c - \sum_{x,y,z} p_{data}(y,x) g(z|x,y) (\log p_{data}(y,x) + \log g(z|x,y)) \\
&\,\,\,\,\,+ \sum_{x,y,z} p_{data}(y,x) g(z|x,y) \log q(y,x) g(z|x,y) \\
&=  c - \sum_{x,y} p_{data}(y,x) \sum_z g(z|x,y) \log g(z|x,y)) \\
&\,\,\,\,\,+ \sum_{x,y} p_{data}(y,x) \sum_z g(z|x,y) \log q(y,x) g(z|x,y) \\
&=  c + E_{p_{data}(y,x)}[H(g(z|x,y))] \\
&\,\,\,\,\,+ \sum_{x=y} p_{data}(y,x) \sum_z g(z|x,y) (\log q(y,x) +\log g(z|x,y)) \\
&=  c + E_{p_{data}(y,x)}[H(g(z|x,y))] \\
&\,\,\,\,+ \sum_{x=y} p_{data}(y,x) \sum_z g(z|x,y) \log q(y,x) \\
&\,\,\,\,-  E_{p_{data}(y,x)}[H(g(z|x,y))] \\
&=  c + \sum_{x=y} p_{data}(y,x)  \log q(y,x) \\
&=  c
-D_{KL}(\,p_{data}(y,x)  \,||
\, q(y,x) \,)  \\
\end{align}$$



$$
\begin{align}
q(y|x) &=  \sum_z q(y,z|x) \\
&\propto \sum_z q(y|z) [ q(z|x) w(z)] \\
q(y|x)&= \sum_z q_r(y|z) q_e(z|x) w(z) / Z_{e,w}(x)\\
Z(x) &= \sum_z q_e(z|x) w(z) \\
q_{eh} &= { q_e(z|x) w(z) \over \sum_z q_e(z|x) w(z)}
\end{align}
$$

重新计算分布间的距离. 这里的采样比较tricky,因为后验分布需要
综合考虑编码器和先验分布,编码出来的分布不能直接使用了,实际操作中,
也可以在归一化的概率 $q_{eh}$ 上采样(高斯卷高斯仍然是高斯). 可以看到
,这样得出的loss形式上与VAE里的KL有区别,主要在于log下面那项.从某种角度讲,VAE的损失的量纲可能是有问题的.这里的两个下界也不等价,第一个对应
在 $q_{eh}$ 上采样,第二个对应在 $q_e$ 上采样,但是要多加一些积分项目

$$
\begin{align}
L_2 &= 
-D_{KL}(p_{data}(y,x)||q(y,x)) \\
&= c + E_{p_{data}(x)}[ \log q(y=x|x)]\\
&= c + E_{p_{data}(x)}[ \log \sum_z q_r(y|z) q_e(z|x) w(z)  - \log Z_{e,w}(x)]\\
&= c + E_{p_{data}(x)}[ \log \sum_z {q_e(z|x) w(z)  \over \sum_z q_e(z|x) w(z) } q_r(y|z) ]\\
&\geq
 E_{p_{data}(x)}[ \sum_z {q_e(z|x) w(z)  \over \sum_z q_e(z|x) w(z) }  \log q_r(y|z) ]\\  
&= E_{p_{data}(x)}[ E_{q_{eh}(z|x)}[\log q_r(y|z) ]]\\  
\dots \\
&-D_{KL}(p_{data}(y,x)||q(y,x)) 
\\&\geq c + E_{p_{data}(x)}[ \sum_z q_e(z|x) \left( \log  { q_r(y|z)w(z)  \over \sum_z q_e(z|x) w(z) } \right) ]
\end{align}
$$

对比一下VAE损失,在原始先验式子上强行加一下KL散度的期望是可以得到的,猜测VAE可能是先尝试了正则项目然后再
倒过来推的损失函数

$$\begin{align}
L_{VAE} &= c+ -D_{KL}(p_{data}(y,x)||q(y,x))  +   E_{p_{data}(y,x)}[-D_{KL}(q(z|x) || w(z))]
\\
&\geq  E_{p_{data}(y,x)}[ E_{q_e(z|x)} [ \log  q_r(y=x|z) ]
  -D_{KL}(q_e(z|x) || w(z))] \\ 
&=  E_{p_{data}(y,x)}\left[ \sum_z q_e(z|x)  \log  q_r(y=x|z) 
+ \sum_z q_e(z|x) (\log w(z) - \log q_e(z|x))  \right] \\
&=  E_{p_{data}(y,x)}\left[ \sum_z q_e(z|x)   
 \left( \log  q_r(y|z) + \log w(z) - \log q_e(z|x) \right )  \right] \\ 
&=  E_{p_{data}(y,x)}\left[ \sum_z q_e(z|x)   
 \left( \log { q_r(y|z) w(z) \over q_e(z|x)} \right )  \right]
\end{align}$$

区别在于括号里log那项的分母上,VAE损失这里用的是编码器给出的分布,
而上式用的是一个配分函数.差的还是有点多.不过分子都乘了一个 $w(z)$ ,
有机会可以对比看看正则化效果. 原则上 $L_2$ 损失因为是Bayesian的,多用
几次自然就收敛到正态分布上了. $L_{VAE}$ 看起来工程性还是比较强,不过算起来可能更加方便


$$\begin{align}
L_2 - L_{VAE} &= E_{p_{data}(x,y)}\log[   { q_r(y|z)w(z)  \over \sum_z q_e(z|x) w(z) } - 
\log { q_r(y|z) w(z) \over q_e(z|x)} ] \\
&= E_{p_{data}(x,y)}\log[   { q_e(z|x) \over \sum_z q_e(z|x) w(z) }   ] \\
\end{align}$$



从第一性原理出发,对一个参数化分布对观测的数据分布进行KL逼近.并且做隐变量分解.首先让我们假设
从隐藏变量解码,可以通过神经网络 $D$ 所对应的条件分布表示 $q_D(x|z)$

$$\begin{align}
L(w) &= - D_{KL}(p_{data}(x) || q_w(x)) \\
q_w(x) &= \sum_z q_w(x,z) 
\\
&= \sum_z q_D(x|z) q_w(z) \\
&= \sum_z q_D(x|z) q_w(z)
\end{align}$$

我们发现概率的计算需要在z上积分. 这还了得! 高维积分是出了名的要人命的啊. 退而求其次,我们尝试去约束后验概率和一个先验分布之间的距离,这样

到这里为止,我们已经引入了生成网络,那么为什么需要编码网络呢?这是因为在 $z$ 上求高维积分是难以直接完成的,
我们希望通过一些技巧,来近似这个高维积分.从直觉上来讲,我们只需要考虑 $q_D(x|z)$ 比较大的那些 $z$,而不用考虑 $q_D(x|z)=0$ 的 $z$

在给定了解码分布以后,我们可以计算一下隐变量的后验概率, 考虑无信息先验,也就是平坦的隐变量空间 $q_w(z)=c$,



$$\begin{align}
q_w(z|x) &= {q_w(x,z) \over \sum_z  q_w(x,z)} \\
& =  {q_w(z) q_D(x|z) \over \sum_z  q_w(z) q_D(x|z)} \\ 
& =  {q_D(x|z) \over \sum_z  q_D(x|z)} 
\end{align}$$





其次根据混合模型中logsumexp的性质,在无其他假设的情况下 $q_w(z)=1$,我们可以用z的最优点估计来估算这个概率


$$\begin{align}
q_w(x) &= \sum_z q_D(x|z) q_w(z) 
\\
&=\exp \log \sum_z q_D(x|z) q_w(z) \\
&=\exp \log \sum_z \exp \log q_D(x|z) \\
&\approx \exp \max_z \log q_D(x|z) \\
\end{align}
$$
