# 3002: 从序列生成看模型隐状态的演进

<http://catsmile.info/3002-seq-hidden-states.html>

前言：本文是炼丹之余对于理论的一些思考，还请大家海涵。最近折腾了半天QK attention，
在toy problem上并没有玩出啥有意思的进展，唯一的收获是模长控制对于attention的
学习至关重要，如果模型能够通过调整模长来影响attention，它很有可能就真的会去这么做。

于是就有了如下的理论思辨文章，毕竟丹虽然炼炸了，改改炼丹的方子还是可以的。

考虑一个简单的生成问题，简单的nlg生成目标可以一步步进行构造
问题的定义对于模型效果至关重要。究竟怎样去定义nlg才可以最大化
应用时的便捷度同时又兼顾架构的灵活度呢？常见的范式是next-token prediction，
这种范式把序列分解成一个诡异的ngram模型，诡异是因为这玩意很不直观，并不是简单地
用一个dag无向图就能描述清楚的。ngram模型的约束在于n词截断，也就是认为给定n个上
文以后，下一词的概率就唯一确定了。但这并不给出下一词的概率计算方法，于是各种五花
八门的模型就有了生长空间。

最暴力的当然是遍历ngram词典的，但是这种遍历模型在n值较大时很容易缺样本。这意味着需要有一种系统的映射，能够从数据中汲取约束，并应用到未见的数据上。

前些时候比较火的rnn以及seq2seq，意图用一个隐藏状态向量hidden state，来利用d维空间，对下一词的分布形成一种描述，再使用某种能量函数和softmax来给出一个分布。需要注意的是，softmax瓶颈理论上会限制d维词嵌入的表现力，也就是只要模型的最后一层套了一个dense到d维加softmax的输出层，都会受到这个瓶颈的限制。

从分解的角度来思考，一个句子具有一些正交的特征维度，比如说实体相关的主题，语气，语法正确性，倾向性，煽动性，等等等等。那么一个有效的非监督学习，必须得要做到对于此类特征的自动推断，才能利用这些隐藏的数据结构对数据进行压缩。也因此，在进行下一词的生成的时候，模型应该考虑平行的多个特征。在给定主题，语气，pos的情况下，应该筛选出对应的符合要求的词语。从这个角度讲，词向量应该要分解成多个平行的维度，在每个维度上单独做内积。这里涉及到一个问题，就是避免跨维度内积相加，而采用最低的内积，是否能够取得更好的筛选效果？这目前不清楚，但是显然是能给平行维度赋予实在意义（不然直接做内积和分开做内积就没差别了）。

从某种角度讲，这种逻辑化的计算更加符合决策的准确性，也可以理解为，用hamming distance来替代euclidean distance，能够得到更符合需求的能量函数和概率分布。使用hamming如果维度1代表语气，维度2代表倾向性，那么在内积（cosine）距离下，维度1的损失可以靠维度2的符合来弥补，而在hamming要求下，维度1的损失，不能靠维度2来弥补。同时hamming也区别于l2范数，在于l2范数在每个维度可以看成一个平方能量函数，要求数字分布在这个均值附近。l2范数看起来跟rbf（radial basis function）是相关的，也曾经是有过大影响的文章呢。

hamming距离的连续形式在mathsexchange（<https://math.stackexchange.com/questions/4390080/can-anyone-suggest-a-continuous-generalization-of-hamming-distance>） 有人用sigmoid给出了构造，最后仍然用到l1范数。事实上，和传统的内积然后softmax相比，这可能增加挺多的计算量。但是至少也是一种突破softmax瓶颈的一种办法吧，毕竟hamming操作的空间是2^d，并不受到矩阵乘法下，秩的限制。从长远来看，hamming距离对于筛查也可能有裨益，看到3个bit的mismatch就可以把向量扔掉了。

以上的讨论，完全可以看成是对于简单嵌入思想embedding的攻击。也就是说，基于单纯矩阵乘法的前向操作，是会产生信息瓶颈的。但是单纯的hamming又意味着每个维度都是binary的，容量太小，所以折衷的办法是构造多向量表征，每个hamming维度用一个向量表征，每个维度内部的相似性用内积来算，最后再用sigmoid和l1进行聚合。

讨论以上的问题，最后还是想得出一个比较合理的rnn架构。对于lstm我还没有深入的考察，但是我感觉它的context向量的很多sigmoid是符合以上的论断的。也就是说，点积距离和hamming距离直接的差异，这可能可以解释rnn的记忆力不如lstm来的好的原因。这个原因就是用sigmoid在相同维度下避开了rank curse的信息瓶颈。

以gpt为代表的transformer模型，说实话，我还没有理解清楚。从直觉上来讲，transformer之所以有效，可能是建立了一个内部模型的先验。如果对transformer的能量函数加以改造，可以近似地认为每个注意力头都在寻找最为合适的那个上家，在找到上家以后继续进行传播，中途经过非线性ffn确保绕开瓶颈。那么这个形式到底对于隐态的表征有什么好处呢？很重要的一点是，这意味着隐态的维度从d，扩大到了n x d。所以在预测下一词的时候，模型能够更加自由地从隐态里面提取信息，很少受到瓶颈的影响。从这个角度来讲，transformer也是一种绕开信息瓶颈的尝试。并且，系统的隐态必须足够大，才能完成诸多transformer所新攻克的任务。从某种角度上，这种模型可以具备复制的机制，来确保隐态的动力学不至于过于复杂。

相比transformer的办法，是否能有办法对于句子做更加显式的建模呢？确实有pcfg的构造尝试，通过对树进行采样来估算似然。事件抽取等任务，本身也是一个关系模型了。现在这些问题都可以用bert等模型进行建模，就说明bert本身是具有了推断实体关系的能力了的。那这种关系，应该还是来自于注意力头。对于有k个头长L的序列的N层网络来说，组合的空间是O(NKLL)，也就是有NKLL条连边需要建立。在每一层，模型都通过平均场计算（前向算子在（由注意力头诱导的图上的分布）的期望）。对于注意力头温度无限低或者hard attention的情况，这个期望就退化成一个单独的样本。可以看到，transformer所诱导出的关系图可能起了重要作用。

但是transformer架构肯定也有边界，最核心的肯定还是这个关系图的诱导模式假设，也就是假设可以从尾节点出发通过一个矩阵寻找最合适的头节点，来形成一个pairwise的作用图。这种图的参数化可能并不是最高效的计算形式。也因此，如果能够对图上的分布进行另外的合理参数化，就有希望对tranformer做出改进。比如说star transformer和set transformer就是一些例子，通过建立共享的中继节点，对关系图，和图上的算子，同时进行分解。那么问题就转化为了，找到图算子的这么一个分解，使得可以对qk的聚合进行近似。问题显然就变成了分解因子肯定会有隐变量，也就是在运行是要显式地解出这些隐变量，我准备再细读一下star和set transformer看看他们做了哪些benchmark。更一般的，任何可以有效定义出一个计算图的操作，可能都可以对神经网络的架构产生进一步的影响。

话说回来，transformer本身的逐层模型，也以意味着整体的算子是逐层叠加的，也就是在n深度方向上，transformer的堆叠形式也是为了近似一个更加复杂的图上的算子。这就能够解释我以前一直困惑的，transformer为什么非要那么深？现在的答案是，如果待拟合的算子最少需要一个ktree来表达，那么transformer就至少要有k层深。

梳理整个发展脉络，可以看到，从暴力的ngram隐态，到简单的d维隐态，再到lstm的binary隐态，再到attention的图隐态，模型在朝着自动建模的方向发展，而且越发地符合一种符号机器的感觉了。我相信图论，矩阵近似，图模型在这里是有一些应用空间的，而连接学派和符号学派的融合将是统计学习接下来的方向。
