#! https://zhuanlan.zhihu.com/p/514109208
# 1009: CSHN 连续态霍氏网络 Continuous State Hopfield Network

[CATSMILE-1009](http://catsmile.info/1009-cshn.html)

- 函数全称: Continuous State Hopfield Network

- 函数解决的问题/不使用的后果: 可以用于理解特定的softmax attention函数,
 可能可以解释self-attention是如何解决指数级相空间的构造和拟合的

- 函数解决改问题的原理: 在一个特定的能量函数上,应用CCCP给出迭代式子.

- 函数可能存在的问题: TBC
- 函数在某一场景下的具体形式:

$$
\begin{aligned}
E(X,y) &= {1\over 2} y^T y - \log(\sum \exp(\beta X^T y)) + C \\
y^{t+1} &= f(y^t) = X \text{softmax}(\beta X^T y )
\end{aligned}
$$

- 函数的具体计算方法: TBC
- 函数备注:

  先是提出了一个类似hopfield energy的能量函数 $H(X,y)= -\text{lse}(\beta, X^T y) + 0.5 y^T y$ ,
  然后略证了这个H函数可以通过一个CCCP迭代法得到局部最优 $Hmin=H(X,y_{min})$ ,并且具有形式 $X \text{softmax}(\beta*X^T y)$ ,称之为Hopfield update rule(HUR)
  但是,之后的操作就比较迷惑了,开始说transformer attention就是hopfield update rule,
  凑了几个矩阵把QK attention重写了一遍,意思说A和B就等同了? 那您的能量函数是走失了吗?....

  所以并没有看到QK Attention作为一个CCCP的严格证明,也没有证明QK Attention不存在一个对应的能量函数, 就很迷惑,
  给出的convergence结果基本上都是对应HUR的,不知道他们为啥有信心把QKA和HUR混同分析...其实这个HUR是比较好猜出来的,
  但是HUR和QKA毕竟有差别,需要一些假设才能确保把QKA表示成CCCP

  终于有一丝曙光把神经网络给物理化了，但是要花多少时间和理论去适配，还是个未知数。
  我个人受这篇文章启发极大，作为一篇连接统计物理和前沿架构的有效尝试，我希望这类工作能够进一步打破次元壁，
  推进AGI研究的前进。个人愚见，基于热力学能量函数对模型的系统性定义，早晚会回到机器学习的核心理论部分，
  正如限制玻尔兹曼机RBM破解了机器学习的低谷期，统计物理会更多地在机器学习中得到应用，
  把机器学习纳入成为物理的一部分，提高炼丹效率。

- 函数参考信源:
    - Hopfield Networks are all you need. <https://arxiv.org/pdf/2008.02217.pdf>
    - LARGE ASSOCIATIVE MEMORY PROBLEM  IN NEURO-BIOLOGY  AND MACHINE LEARNING, 2021, Dmitry Krotov, John Hopfield
    <https://arxiv.org/pdf/2008.06996.pdf>
    - Hierarchical Associative Memory, 2020, Dmitry Krotov <https://arxiv.org/pdf/2107.06446.pdf>
    - The Concave-Convex Procedure (CCCP). NIPS 2001, Alan L. Yuille, Anand Rangarajan. <https://papers.nips.cc/paper/2001/file/a012869311d64a44b5a0d567cd20de04-Paper.pdf>
