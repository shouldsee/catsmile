#! https://zhuanlan.zhihu.com/p/555924600
# 1032: 变分自编码器 Variational Autoencoder

[CATSMILE-1032](http://catsmile.info/1032-vae.html)


```{toctree}
---
maxdepth: 4
---
1032-vae.md
```

## 前言

- 目标: 
- 背景与动机: 
    - 单独整理VAE的损失形式
    - [TBC,讨论VAE的不同先验形式]
- 结论: 
    - 本文从ImportanceSampling出发建立了VAE的损失形式,并指出使用Jensen不等式得到的ELBO具有引导编码器的作用. 并证明,未使用ELBO的优化目标无法为编码器提供梯度, 从而说明了ELBO的必要性.
    - 附带了KL联合概率形式的损失. 因为形式相同,所以KL损失也同样具有引导编码器的作用
- 备注: 
- 完成度: 低
- 关键词: 
- 展望方向:
- 相关篇目:
    - [CATSMILE-1019](./1019-vaml) 机器学习变分近似
    - [CATSMILE-1004](./1004-diff-expectation) 期望函数的梯度
- CHANGELOG:
    - 20220827 加入EBLO的导数.加入Baseline降低方差的计算
    - 20220826 加入JensenImportanceSampling视角
    - 20220820 INIT FROM CATSMILE-1019


VAE是变分推断思想和神经网络参数化形式的完美结合,可以说是概率图模型和神经网络模型中间的一座桥梁.DDPM也从跟VAE有深刻联系

### VAE = ImportanceSampling + Jensen's Inequality

VAE的ELBO也可以很简单地理解为重要性采样的一个下界. 

$$\begin{aligned}
p(x) &= \int_z q(z) q_r(x|z) .dz
\\ &= \int_z q(z) q_e(z|x){q_r(x|z) \over q_e(z|x)} .dz
\\ &= \int_z  q_e(z|x){ q(z) q_r(x|z) \over q_e(z|x)} .dz
\\ &= E_{q_e(z|x)}[ {q(z) q_r(x|z) \over q_e(z|x)} ]
\\ \log p(x) &\geq E_{q_e(z|x)}[\log { q(z) q_r(x|z) \over q_e(z|x)} ]
\end{aligned}$$

根据Jensen不等式,等号在  ${q(z) q_r(x|z) \over q_e(z|x)}= k$ 处处成立时取到. 此时易证变分后验即是贝叶斯最优后验

$$
q_e(z|x) = { q(z) q_r(x|z)  \over \int_z q(z) q_r(x|z) }
$$


### IS求导: 无下界形式

IS对于编码器的导数应该是零,因为IS不改变期望的值,只改变其方差.所以期望不受ProposalDistribution/EncodingDistribution/WeightDistribution的影响,选哪个都一样.

$$\begin{aligned}
p(x) &= E_{q_e(z|x)}[ {q(z) q_r(x|z) \over q_e(z|x)} ]
\\
m &= \text{param}(q_e)\\
\nabla_m p(x) 
&= \nabla_m  E_{q_e(z|x)}[ {q(z) q_r(x|z) \over q_e(z|x)} ]
\\ &= \nabla_m  \int_z [q_e(z|x) {q(z) q_r(x|z) \over q_e(z|x)} ]
\\ &=   \int_z \nabla_m[q_e(z|x) {q(z) q_r(x|z) \over q_e(z|x)} ]
\\ &=   \int_z  q_e(z|x)\nabla_m \left[ \log q_e(z|x) sg({q(z) q_r(x|z) \over q_e(z|x)} ) + {q(z) q_r(x|z) \over q_e(z|x)} \right] 
\\ &=   \int_z  q_e(z|x) \nabla_m \left[ \log q_e(z|x) sg({q(z) q_r(x|z) \over q_e(z|x)} )\right] 
\\ &\dots + \int_zq_e(z|x) \nabla_m \left[ {q(z) q_r(x|z) \over q_e(z|x)} \right] 
\\ &=   \int_z \nabla_m \left[ \log q_e(z|x) sg({q(z) q_r(x|z) \over 1} )\right] 
\\ &\dots + \int_z q_e(z|x) \left[ {- q(z) q_r(x|z) \nabla_m q_e(z|x)  \over q_e^2(z|x)} \right] 
\\ &=   \int_z {q(z) q_r(x|z)} \nabla_m \left[ \log q_e(z|x) \right] 
\\ &\dots + \int_z  -q(z) q_r(x|z) {\nabla_m q_e(z|x)  \over q_e(z|x)}
\\ &=   \int_z {q(z) q_r(x|z)} \left[ \nabla_m \log q_e(z|x) 
-{\nabla_m q_e(z|x)  \over q_e(z|x)} \right] 
\\ &=   \int_z {q(z) q_r(x|z)}  \cdot 0 
\\ &=   0
\end{aligned}$$

### IS求导: ELBO下界的导数

至少从表面上看,这个导数并不一定为0了. 这说明ELBO不光是起到了一个下界的作用?至少是为编码器提供了明确的优化方向?根据Jensen不等式,ELBO在 $q_e(z|x)$ 取到真实后验的时候最大化,取到 $\log p(x)$ ,所以我们可以认为ELBO提供的导数,提供了一个缩小变分后验到真实后验的距离的方向

$$\begin{aligned}
\\ \log p(x) &\geq ELBO= E_{q_e(z|x)}[\log { q(z) q_r(x|z) \over q_e(z|x)} ]
\\
m&=e
\\ \nabla_m ELBO &= \int_z \nabla_m \left [q_e(z|x) \log { q(z) q_r(x|z) \over q_e(z|x)} \right ]
\\&= \int_z q_e(z|x) \nabla_m [ \log q_e(z|x) sg( \log { q(z) q_r(x|z) \over q_e(z|x)} ) 
\\ &\dots + \log { q(z) q_r(x|z) \over q_e(z|x)} ] 
\\&= \int_z q_e(z|x) \nabla_m [ \log q_e(z|x) sg( \log { q(z) q_r(x|z) \over q_e(z|x)} ) 
\\ &\cdots - \log { q_e(z|x)}  ] 
\\&= \int_z q_e(z|x) \nabla_m \left[ \log q_e(z|x) sg( \log { q(z) q_r(x|z) \over q_e(z|x)} -1 )\right ] 
\\&= E_{q_e(z|x)}  \left[ \nabla_m \log q_e(z|x) sg( \log { q(z) q_r(x|z) \over q_e(z|x)} -1 )\right ] 
\\&= \int_z  \nabla_m \left[ q_e(z|x)  sg( \log { q(z) q_r(x|z) \over q_e(z|x)} -1 )\right ] 
\\&= \nabla_m \int_z    q_e(z|x)  sg( \log { q(z) q_r(x|z) \over q_e(z|x)} -1 )
\end{aligned}$$

可以看到,如果 $\log { q(z) q_r(x|z) \over q_e(z|x)} -1 = c$ ,那么就会确保ELBO的导数为0. 进一步的(其实是因为这个1实在太突兀了),我们可以观察到sg里面的常数项积分后得到是0.


$$\begin{aligned}
& \int_z q_e(z|x) \nabla_m \left[ \log q_e(z|x)\cdot c  \right ] 
\\&=  \int_z \nabla_m \left[ q_e(z|x) \cdot c \right]
\\&=  \nabla_m \int_z  c \cdot   \left[ q_e(z|x)\right]
\\&=  0
\end{aligned}$$

$$\begin{aligned}
\\ \nabla_m ELBO &= \int_z \nabla_m \left [q_e(z|x) \log { q(z) q_r(x|z) \over q_e(z|x)} \right ]
\\&= \int_z q_e(z|x) \nabla_m [ \log q_e(z|x) sg( \log { q(z) q_r(x|z) \over q_e(z|x)} ) 
\\ &\dots + \log { q(z) q_r(x|z) \over q_e(z|x)} ] 
\\&= \int_z q_e(z|x) \nabla_m [ \log q_e(z|x) sg( \log { q(z) q_r(x|z) \over q_e(z|x)} ) 
\\ &\dots - \log {q_e(z|x)} + \log q(z) q_r(x|z)] 
\\&= \int_z q_e(z|x) \nabla_m [\log q_e(z|x) sg( \log { q(z) q_r(x|z) \over q_e(z|x)} ) 
\\ & \dots + \log q(z) q_r(x|z) ] 
\\& \dots- \int_z q_e(z|x) \nabla_m \log {q_e(z|x)} 
\\&= \int_z q_e(z|x) \nabla_m [\log q_e(z|x) sg( \log { q(z) q_r(x|z) \over q_e(z|x)} ) 
\\ & \dots + \log q(z) q_r(x|z) ] - 0
\\&= E_{q_e(z|x)} \nabla_m \left[ \begin{aligned} & \log q_e(z|x) sg( \log { q(z) q_r(x|z) \over q_e(z|x)} ) 
\\ &\dots + \log q(z) q_r(x|z)  \end{aligned}\right] 
\end{aligned}$$

观察最后的这个形式,可以发现只要似然比为定值,那么关于编码器参数的梯度就一定为零. 如果,这个似然比不是常数,那么在 $q_e(z|x)$ 低于后验期望的地方,采样概率会上升,在高于后验期望的地方,采样概率会下降. 当然这个要在概率归一化的情景下才能讨论. 而对于解码器,其目标就是尽量提高在编码分布上的解码似然的期望.所以ELBO用一个损失,同时提供了编码器和解码器的优化目标,可以说是非常方便了

### IS求导: 用Baseline方法降低Variance

事实上,根据additive variate方法, 我们可以在sg内部加一个baseline,这样应该可以降低梯度估计值的variance. 

因为我们知道一阶矩是不变的,所以考察方差时,只关注二阶矩就可以. 

$$\begin{aligned}
Var(X)&=E[(X - E(X))^2] 
\\ &= E[X ^2  - 2 X \cdot E(X) + E^2[X]]
\\ &= E[X ^2] - E^2[X]
\end{aligned}$$

我们观察一下二阶矩对c的导数看看. 

$$\begin{aligned}
&\nabla_c E[X^2] 
\\ &= \nabla_c E_{q_e(z|x)} \left[ \begin{aligned} &  (\log { q(z) q_r(x|z) \over q_e(z|x)} -c )^2 (\nabla_m  \log q_e(z|x))^2  \end{aligned}\right] 
\\ &=  E_{q_e(z|x)} \left[ \begin{aligned} & (\nabla_m  \log q_e(z|x))^2 \nabla_c (\log { q(z) q_r(x|z) \over q_e(z|x)} -c )^2   \end{aligned}\right] 
\\ &=  2 E_{q_e(z|x)} \left[ \begin{aligned} & (\nabla_m  \log q_e(z|x))^2  (c - \log { q(z) q_r(x|z) \over q_e(z|x)} )   \end{aligned}\right] 
\end{aligned}$$

事实上,我们可以求出导数为零的那个点. 可以看出也具有一个IS的形式

$$\begin{aligned}
\\&-\nabla_c E[X^2] = 0
\\ 0 &=  2 E_{q_e(z|x)} \left[ \begin{aligned} & (\nabla_m  \log q_e(z|x))^2  (\log { q(z) q_r(x|z) \over q_e(z|x)} - c)   \end{aligned}\right] 
\\ 0 &=   \left(\begin{aligned} &  E_{q_e(z|x)}[ (\nabla_m  \log q_e(z|x))^2  \log { q(z) q_r(x|z) \over q_e(z|x)} ]
\\& \dots - c\cdot  E_{q_e(z|x)}[ (\nabla_m  \log q_e(z|x))^2 ]  \end{aligned}\right) 
\\ c &={ E_{q_e(z|x)}[ (\nabla_m  \log q_e(z|x))^2  \log { q(z) q_r(x|z) \over q_e(z|x)} ] \over E_{q_e(z|x)}[ (\nabla_m  \log q_e(z|x))^2 ]}
\end{aligned}$$

观察一下二阶导数,我们知道最小值点的二阶导为正.而这个函数的二阶导是非负的. 因此我们可以确认以上的驻点是全局最小值.

$$\begin{aligned}
\\& \nabla_c^2 E[X^2] 
\\ &=  2 \nabla_c E_{q_e(z|x)} \left[ \begin{aligned} & (\nabla_m  \log q_e(z|x))^2  ( c - \log { q(z) q_r(x|z) \over q_e(z|x)} ) \end{aligned}\right] 
\\ &=  2  E_{q_e(z|x)} \left[ \begin{aligned} & (\nabla_m  \log q_e(z|x))^2  ( 1 -0) \end{aligned}\right] 
\\ &=  2  E_{q_e(z|x)} \left[ \begin{aligned} & (\nabla_m  \log q_e(z|x))^2   \end{aligned}\right] 
\\ & \geq 0
\end{aligned}$$

但是要用这个baseline,只采样一次肯定是不行的,因为单样本给出的估计值插回去就得到梯度为零了.另外,这个baseline是针对单个参数计算的,对于多个参数,需要重新考虑其多维向量形式.如果只考虑trace意义上的方差,那么其实把平方项替换成L2norm就好.从形式上看,这权重类似fisher信息矩阵的迹.

$$\begin{aligned}
&\nabla_c \sum_m E[X^2] 
\\ &=  2 E_{q_e(z|x)} \left[ \begin{aligned} & \sum_m ( \nabla_m  \log q_e(z|x))^2  (c - \log { q(z) q_r(x|z) \over q_e(z|x)} )   \end{aligned}\right] 
\\ &=  2 E_{q_e(z|x)} \left[ \begin{aligned} &  || \nabla_m  \log q_e(z|x) ||^2  (c - \log { q(z) q_r(x|z) \over q_e(z|x)} )   \end{aligned}\right] 
\end{aligned}$$


$$\begin{aligned}
c &={ E_{q_e(z|x)}[ ||\nabla_m  \log q_e(z|x))||^2  \log { q(z) q_r(x|z) \over q_e(z|x)} ] \over E_{q_e(z|x)}[ ||\nabla_m  \log q_e(z|x)||^2 ]}
\end{aligned}$$

### 讨论: VAE的无噪声形式?

本节回答为什么VAE一定要引入噪声? 结论:噪声确保重要性采样等式成立 <https://stats.stackexchange.com/a/247778/187914>

在实际使用中,降低采样的方差是一个提高算法效率的通用方法. 但是如果条件分布 $q_e$ 是没有方差的dirac分布,那么就会需要处理dirac意义无穷大的 $\log q_e(z|x)$ 那么. 即便我们忽略掉这一项,考察剩余的两项, 并假定生成模型 $q(z) q_r(x|z)$ 已知,那么最优的dirac编码器,显然是把z放置在后验概率最高的点的那个函数. 这点在[CATSMILE-1014](./1014-gae)里已经做过一点探讨. 

$$
\hat{q}_e(z|x) = \text{argmax}_z  q(z) q_r(x|z)
$$

也就是说,在给定生成模型的情况下,解出最优编码器,就是要使得编码后的那些点的后验概率尽量的大.同时,
给定编码器,解出生成模型的目标,也是要让这些编码点的后验概率尽可能的大.这个过程是否会引起编码点坍塌到一个低方差的高斯分布呢? 这个情况下, $q(z)$ 远远大于 $q_r(x|z)$ ,后验概率完全由先验概率主导.这需要实际实验才能佐证. 比较头疼的点在于,边际概率由于 $q_e(z|x)$ 无穷大的存在,变得无穷小了. 这似乎可以作为引入噪声的一个必要原因, 有了噪声后,这个期望又变得可以计算了. 但是在离散的情景下面,这似转化成为重要性采样的覆盖率问题,如果 $q(z)$ 和 $q_e(z|x)$ 的支撑集没啥交集,那么重要性采样的等式就因为要除以0而不再成立了. 为了确保重要性采样成立,必须引入噪声

$$\begin{align}
& E_{q(z)}[q_r(x|z)] \\
& =E_{q_e(z|x)}[{q(z) q_r(x|z) \over q_e(z|x)}]
\end{align}$$


考虑对生成模型的优化过程,生成模型只能看见离散的编码点,这有可能导致生成模型只在这里离散的点上才有效,而一旦脱离了这些点就开始胡言乱语.但是之前的一系列实验也表明,网络本身拓扑对于压缩效率的影响,在无噪情况下是容易比较的,也因此我们可能用点估计就可以干很多事情

如果VAE的噪声是为了对抗过拟合,那么我们最好能直接观测噪声对于过拟合的对抗作用,否则又为什么要引入噪声来降低模型的效率呢? 我们知道Bayesian学派爱死了一个大于点估计的后验分布,但是这确确实实是会把理论变得复杂许多. 但是另一方面,加入噪声,很有可能确保了模型隐变量空间的连续有效性,从而避免了生成模型只在特定点位有效.

但是优化ELBO仍然有一个问题,我们并没有办法保障变分后验接近真实后验. 这个gap是ELBO无法捕捉到的,可能要加入一项正则项加以约束. 我们希望最大化ELBO的同时,减小其估算方差.但是这个平方项目似乎没啥直接求导的好办法...

$$\begin{aligned}
\log p(x) &\geq E_{q_e(z|x)}[\log { q(z) q_r(x|z) \over q_e(z|x)} ]
\\ 
&\text{argmax}  [-Var_{q_e(z|x)}[\log { q(z) q_r(x|z) \over q_e(z|x)}]]\\
&\text{argmax}  [-E_{q_e(z|x)}[\log^2 { q(z) q_r(x|z) \over q_e(z|x)}] + E_{q_e(z|x)}^2[\log { q(z) q_r(x|z) \over q_e(z|x)} ]]
\end{aligned}$$


### 联合分布的KL形式

VAE的损失是联合概率之间的KL损失,且左右两侧的概率分解形式并不一样. 左侧分解成数据和编码器 $q_e(z|x)$, 在右边直接用了生成模型, 先生成编码,再解码成数据. 

注意对期望函数求导的时候,有一些技巧,确保梯度的正确性.

$$
\begin{align}
&L_{vae} \\
&= -D_{KL}(p_{data}(x,z)||q(x,z)) 
\\ &= -D_{KL}(p_{data}(x)q_e(z|x)||q_r(x|z)w(z)) 
\\
&= c + \sum_{x,z} p_{data}(x)q_e(z|x) \log {q_r(x|z)w(z)
\over p_{data}(x)q_e(z|x) } \\
&= c + \sum_{x} p_{data}(x) \sum_z q_e(z|x) \log {q_r(x|z)w(z)
\over q_e(z|x) } \\
&= c + \sum_{x} p_{data}(x) \sum_z q_e(z|x) \log q_r(x|z) + \sum_z q_e(z|x) \log {w(z) \over q_e(z|x) } \\
&= c + E_{p_{data}(x)} \left [ E_{q_e(z|x)}[ \log q_r(x|z)] - D_{KL}( q_e(z|x)  || w(z) )  \right ]\\
&= c + E_{p_{data}(x)} \left [ E_{q_e(z|x)}[ \log q_r(x|z)- \log q_e(z|x) + \log w(z) ]\right ]\\
\end{align}
$$

### 联合概率的KL是边际概率KL的下界

对比一下联合分布和边缘分布的区别. 在左侧,分布直接退化为数据分布

$$
\begin{align}
L_{marg} &= -D_{KL}(\sum_z p_{data}(x)q(z|x) ||
\sum_z w(z)  q_r(x|z)  ) \\
&=  \sum_x  \sum_z p_{data}(x) q_e(z|x) \left( \log
 \sum_z w(z)  q_r(x|z)  - \log \sum_z p_{data}(x) q_e(z|x) \right ) \\
&= c + \sum_x   p_{data}(x) \sum_z q_e(z|x) \left( \log
 \sum_z w(z)  q_r(x|z)  - \log \sum_z  q_e(z|x) \right ) \\
&= c + \sum_x   p_{data}(x) \sum_z q_e(z|x) \left( \log
 {\sum_z w(z)  q_r(x|z)  \over   \sum_z  q_e(z|x) }\right ) \\
&= c + \sum_x   p_{data}(x) \sum_z q_e(z|x) \left( \log
 {\sum_z w(z)  q_r(x|z)  \over   1 }\right ) \\
\end{align}
$$

直接一眼乍看上去不太能看出来差距有多少,尝试做差

$$
\begin{align}
& L_{vae} -L_{marg} 
\\&= c + \sum_{x,z} p_{data}(x)q_e(z|x) \log {q_r(x|z)w(z)
\over p_{data}(x)q_e(z|x) } 
\\ &-  \sum_{x,z}   p_{data}(x)  q_e(z|x) \left( \log
 {\sum_z w(z)  q_r(x|z)  \over   p_{data}(x) \sum_z  q_e(z|x) }\right )\\
 &= c + \sum_{x,z} p_{data}(x)q_e(z|x) \left (\log {q_r(x|z)w(z)
\over p_{data}(x)q_e(z|x) } - \log {\sum_z w(z)  q_r(x|z)  \over   p_{data}(x) \sum_z  q_e(z|x) }\right )\\
 &= c + \sum_{x,z} p_{data}(x)q_e(z|x) \left (\log {q_r(x|z)w(z)
\over \sum_z w(z)  q_r(x|z)    } - \log {q_e(z|x) \over   \sum_z  q_e(z|x) }\right )\\
 &= c + \sum_{x} p_{data}(x) \sum_z q_e(z|x) \left (\log {q_r(x|z)w(z)
\over \sum_z w(z)  q_r(x|z)    } - \log {q_e(z|x) \over   1}\right )\\
&= c - \sum_{x} p_{data}(x) D_{KL}(q_e(z|x) || {q_r(x|z)w(z)
\over \sum_z w(z)  q_r(x|z) })\\
&\leq 0 \\
L_2 &\leq  L_3
\end{align}
$$

可以看到联合分布的负KL,严格小于边际分布的负KL,而且其中的差别在于左侧
编码分布 $q_e(z|x)$ 和右侧生成模型自然引导出的隐变量后验分布之间的KL散度.也就是说,对于一个完美的编码器,这两项取到等号


## 参考

- 苏剑林变分推断新解 <https://zhuanlan.zhihu.com/p/40105143>或<https://kexue.fm/archives/5716>

- 苏剑林. (Dec. 09, 2021). 《变分自编码器（八）：估计样本概率密度 》[Blog post]. Retrieved from <https://kexue.fm/archives/8791>

- KingmaWelling2013: Auto-Encoding Variational Bayes :<https://arxiv.org/abs/1312.6114>

- Greensmith2004: Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning <https://www.jmlr.org/papers/volume5/greensmith04a/greensmith04a.pdf>