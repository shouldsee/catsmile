#! https://zhuanlan.zhihu.com/p/529088417
# 1015: 贝叶斯自编码器 (BAE) Bayesian AutoEncoder 


[CATSMILE-1015](http://catsmile.info/1015-bae.html)

## 前言

- 目标: 从理论的角度出发,观察概率性自编码器的函数特点
- 关键词: 贝叶斯定理，配分函数，模型选择，和最大似然
- 动机: 
  - 从度量约束的自编码问题出发，我们发现，如PCA等的自编码器可以被视作完全确定的自编码器,而GMM,VAE为代表的,则是用隐变量的后验分布来对输入进行编码.
  直接将GMM,VAE与PCA进行对比,会由于记号上的差异,而难以发现其损失函数的相似性.我们通过下文说明,这是由于绝对零度和有温场景下的符号差异造成的
  - 给出了模型参数上的后验分布的显式定义,从理论上扩展了梯度下降意义上的最优化为采样过程.
- 可能问题: 需要有高效的采样过程
- 相关特例模型: GMM,VAE,RBM,PCA
- 展望: 
  - 借助自编码过程,和概性自编码过程的对应,研究自然科学中可能的自编码过程.因为实际系统很少有绝对零度的系统.但是不排除基于理论力学的系统可以表现出全定性的自编码过程
  - 更加细致地定义Bayesian的意义,防止滥用概念
  
  
我们知道,在相同的能量函数下,将温度降为零会逼近极大似然的框架,编码变为全定的(deterministic),如果温度不为0,则编码过程具有随机性. 我称这种使用随机编码的自编码器为贝叶斯自编码器(Bayesian AutoEncoder).   
我们可以简单地记
数据x,模型m,隐变量z的能量函数为,在下文中暂时省略
了有关温度的记号

$$
p(x,m,z）\propto \exp H(x,m,z)
$$

$$Hmax(x,m)=\text{logsumexp}_z H(x,m,z)$$

继续对x上求平均（严格工程意义上应该求最小值）

$$Hmaxe(m,X)= {1\over N} \sum_x \text{logsumexp}_z H(x,m,z)$$

最后我们对m取软极大值

$$Hmaxemax(X)= \text{logsumexp}_m  {1\over N} \sum_x \text{logsumexp}_z H(x,m,z)$$

这个函数就是按照 $p(x,m,z)$　分布的态所对应的配分函数，注意到这个函数在X固定时是一个常量，仅仅取决于H的形式。如果　logsumexp的逆温度取无穷大，那么这个值就退化成GAE的负损失函数上界。所不同之处在于，这里考虑的都是采样。配分函数主要规定的是如何对各种统计量进行采样。

绝对零度情况下，概率退化成狄拉克分布，平滑算子退化成
极值算子，BAE损失退化成GAE损失,GAE已经在[CATSMILE-1014](./1014-gae.md)加以定义过

$$
Hmem(X)= \max_m  {1 \over N} \sum_x \max_z H(x,m,z)
$$

回到有温度情况,让我们尝试对m微分一下.

$$\begin{align}
 {\partial \over \partial m} HmaxEmax(X) &=\sum_m p(m) {1\over N} \sum_x \sum_z p(z|x,m) {\partial \over \partial m} H(x,m,z)\\
&=\sum_m p(m) {1\over N} \sum_x \sum_z p(z|m,x) {\partial \over \partial m} H(x,m,z) \\
&=\sum_m p(m) {1\over N} \sum_x E(p(z|m,x))[{\partial \over \partial m} H(x,m,z)]
\end{align}
$$

其中 p(m) 是 m的边缘概率。这里计算的是给定x和m的条件概率下H函数的梯度的期望。但是由于我们求导的是一个定值，所以理论上这个式子应当是等于0的，也就是说在不同概率的模型m下，编码概率上m调整的在数据上的期望是平衡的。

因为我们想要观察对于m应当如何优化，所以应把m从求和式中解放出来，退回到单个m的情况,这个时候对m进行求导，其实就是把上一个式子前面关于m的期望去掉


$$Hmaxe(m,X)= {1\over N} \sum_x \text{logsumexp}_z H(x,m,z) \\
{\partial \over \partial m} Hmaxe = {1\over N} \sum_x \sum_z p(z|m,x) {\partial \over \partial m} H(x,m,z)
$$

我们可以看到，让m的对数似然更优的方向，需要对每个样本在编码分布，求H梯度的期望。也就是，z的编码效果越好，它对应的H梯度就越重要。因此可以把 $HmaxE(m)$ 看做是衡量m好坏的一个能量，也就意味着在它所对应的分布上进行采样，就能让更好的模型，具有更大的概率。特别的，如果温度为0，那么m的分布退化到极大似然的点估计。对比em算法中的期望步和最大化步，也是在给定后验编码分布时（期望步），对m做最优化（最大化步），而且这里使用的恰好就是loglikelihood的梯度。em是关于m优化sumlog作为logsum的下界，这里是沿着sumlog的梯度优化m，因此是联系紧密的。实际操作中，计算p(z|m,x)，就对应着em中的e-step。

接着计算一下这个能量对自己的期望，也就是用到更大的配分函数。有理由认为，这个熵是关于H的一个泛函，最大化/最小化这个负熵，应当对应着模型架构L的一些性质。可以看到，这个期望越大，说明平均的m更好，也就是更能够为m的倾向性提供信息。如果到达最大熵，也就是最小负熵极限，就意味着在m上退化成均匀分布，也就是无论怎么调整m，都不能让模型变得更好。

$$G(H)= \sum_m  HmaxE(m) \exp(HmaxE(m)-Hmaxemax)$$

我认为，我们感兴趣的应当还是泛函Hmaxemax(X,H)，因为它近似表征了在给定H分布下H所能取到的极大值，也就是（最好的模型有多好）。这一般意味着把函数H映射成一些参数，然后再在这个参数上采样。

或者说，如果把p(m)视作一族函数上的分布，那么Hmaxemax(X)就近似表征这族函数上能取到的最优模型。于是模型选择，就完全转化成了在p（m）上采样，而恰当的参数化，应当给出一个低熵分布来确保采样有意义。如果能把这种采样应用到神经网络里，可能可以利用统计物理计算一些函数族m的性质。

这两种思考角度取决于m所覆盖的参数空间,如果把m并入H里 $H(x,m,z)=H_m(x,z)$那么,我们就可以尝试说,$HmaxEmax$ 是 $Hmaxe(H_m)$ 泛函的在热力学意义上的极值点

## 应用

给定数据和函数族 $X,H_m$ ,在 $p(m)$ 上采样得到的样本,就是该函数族概率意义上的最优自编码器. 比如如果 $H_m(x,z)$ 是一个GMM模型,那么 $p(m)$
就对应着用独热编码上的分布对输入的狄拉克样本分布进行最优编码的模型.可以通过采样 $p(m)$ ,或研究梯度${\partial \over \partial m} Hmaxe$,或 $G(H_m)$ 
来研究这族模型的性质.

## 结论

对于贝叶斯模型，我们可以通过构造一系列的Hmax来达成能量函数的转化。这样可以在不引入VI变分近似的情况下,完全从最优化
的角度思考纯粹的自编码过程。事实上在压缩的情景里，从狄拉克分布的x到编码分布z的映射，直接由H(x,m,z)决定。解码就意味着在这个p(z|x,m)q(x)吸收了有关x的信息后，再把z积分掉，就又得到了解码出的x上的分布。

也就是说，这个框架，有助于思考泛函化的Bayes推断的理论性质，比如自编码是怎样传递信息的。那么实际操作中最大的问题就是要选择函数族m，并在积分的操作难度，和最优模型的理论上限之间达到平衡。如果选择了高斯的函数族m，那就可以唯一确定p(z|x,m)的性质。也就是只需要确定一个编码能量H，然后通过适当的积分就能得出编码分布和解码分布之间的转化关系。这个框架也可以用来直接推导降噪自编码器DAE的一些性质.总之,一句话总结,就是概性的自编码过程可以通过 $\text{logsumexp}$ 算子替代 $\max$ ,来加以描述.



## 草稿

### 对称的编码解码过程

给定x,最优编码损失为

$$
\text{logsumexp}_z H(x,m,z)
$$

给定z,最优编码损失为

$$
\text{logsumexp}_ H(x,m,z)
$$


### 取最差编码样本

从工程要求出发，我们考虑优化最差编码，其连续形式借助logsumexp实现。这对于构建模型的持续学习能力，说不定有一些意义。

$$
Hmaxminmax(X)= \text{logsumexp}_m (- \text{logsumexp}_x (-\text{logsumexp}_z H(x,m,z)))\\
Hmaxminmax(X)= \text{logsum}_m (1 / (\text{sum}_x 1/(\sum_z \exp H(x,m,z))) )
$$

也就是用到了指数空间的调和平均数,极限形式如下

$$
Hmmm(X) = \max_m \min_x \max_z H(x,mz)
$$

### 配分函数

$$Z=\sum \exp H(x,m,z)
$$

其中H(x,m,z)可以理解为在模型m下z恢复出的数据距离x的邻近程度。对于每个x，我们考虑依照 P 意义的分布下，H取得软极大值，对H(z)微分会得出p(z)的局部概率
