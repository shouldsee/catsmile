---
---

@inproceedings{holdgraf_evidence_2014,
	address = {Brisbane, Australia, Australia},
	title = {Evidence for {Predictive} {Coding} in {Human} {Auditory} {Cortex}},
	booktitle = {International {Conference} on {Cognitive} {Neuroscience}},
	publisher = {Frontiers in Neuroscience},
	author = {Holdgraf, Christopher Ramsay and de Heer, Wendy and Pasley, Brian N. and Knight, Robert T.},
	year = {2014}
}

@article{holdgraf_rapid_2016,
	title = {Rapid tuning shifts in human auditory cortex enhance speech intelligibility},
	volume = {7},
	issn = {2041-1723},
	url = {http://www.nature.com/doifinder/10.1038/ncomms13654},
	doi = {10.1038/ncomms13654},
	number = {May},
	journal = {Nature Communications},
	author = {Holdgraf, Christopher Ramsay and de Heer, Wendy and Pasley, Brian N. and Rieger, Jochem W. and Crone, Nathan and Lin, Jack J. and Knight, Robert T. and Theunissen, Frédéric E.},
	year = {2016},
	pages = {13654},
	file = {Holdgraf et al. - 2016 - Rapid tuning shifts in human auditory cortex enhance speech intelligibility.pdf:C\:\\Users\\chold\\Zotero\\storage\\MDQP3JWE\\Holdgraf et al. - 2016 - Rapid tuning shifts in human auditory cortex enhance speech intelligibility.pdf:application/pdf}
}

@inproceedings{holdgraf_portable_2017,
	title = {Portable learning environments for hands-on computational instruction using container-and cloud-based technology to teach data science},
	volume = {Part F1287},
	isbn = {978-1-4503-5272-7},
	doi = {10.1145/3093338.3093370},
	abstract = {© 2017 ACM. There is an increasing interest in learning outside of the traditional classroom setting. This is especially true for topics covering computational tools and data science, as both are challenging to incorporate in the standard curriculum. These atypical learning environments offer new opportunities for teaching, particularly when it comes to combining conceptual knowledge with hands-on experience/expertise with methods and skills. Advances in cloud computing and containerized environments provide an attractive opportunity to improve the effciency and ease with which students can learn. This manuscript details recent advances towards using commonly-Available cloud computing services and advanced cyberinfrastructure support for improving the learning experience in bootcamp-style events. We cover the benets (and challenges) of using a server hosted remotely instead of relying on student laptops, discuss the technology that was used in order to make this possible, and give suggestions for how others could implement and improve upon this model for pedagogy and reproducibility.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	author = {Holdgraf, Christopher Ramsay and Culich, A. and Rokem, A. and Deniz, F. and Alegro, M. and Ushizima, D.},
	year = {2017},
	keywords = {Teaching, Bootcamps, Cloud computing, Data science, Docker, Pedagogy}
}

@article{holdgraf_encoding_2017,
	title = {Encoding and decoding models in cognitive electrophysiology},
	volume = {11},
	issn = {16625137},
	doi = {10.3389/fnsys.2017.00061},
	abstract = {© 2017 Holdgraf, Rieger, Micheli, Martin, Knight and Theunissen. Cognitive neuroscience has seen rapid growth in the size and complexity of data recorded from the human brain as well as in the computational tools available to analyze this data. This data explosion has resulted in an increased use of multivariate, model-based methods for asking neuroscience questions, allowing scientists to investigate multiple hypotheses with a single dataset, to use complex, time-varying stimuli, and to study the human brain under more naturalistic conditions. These tools come in the form of “Encoding” models, in which stimulus features are used to model brain activity, and “Decoding” models, in which neural features are used to generated a stimulus output. Here we review the current state of encoding and decoding models in cognitive electrophysiology and provide a practical guide toward conducting experiments and analyses in this emerging field. Our examples focus on using linear models in the study of human language and audition. We show how to calculate auditory receptive fields from natural sounds as well as how to decode neural recordings to predict speech. The paper aims to be a useful tutorial to these approaches, and a practical introduction to using machine learning and applied statistics to build models of neural activity. The data analytic approaches we discuss may also be applied to other sensory modalities, motor systems, and cognitive systems, and we cover some examples in these areas. In addition, a collection of Jupyter notebooks is publicly available as a complement to the material covered in this paper, providing code examples and tutorials for predictive modeling in python. The aimis to provide a practical understanding of predictivemodeling of human brain data and to propose best-practices in conducting these analyses.},
	journal = {Frontiers in Systems Neuroscience},
	author = {Holdgraf, Christopher Ramsay and Rieger, J.W. and Micheli, C. and Martin, S. and Knight, R.T. and Theunissen, F.E.},
	year = {2017},
	keywords = {Decoding models, Encoding models, Electrocorticography (ECoG), Electrophysiology/evoked potentials, Machine learning applied to neuroscience, Natural stimuli, Predictive modeling, Tutorials}
}

@book{ruby,
  title     = {The Ruby Programming Language},
  author    = {Flanagan, David and Matsumoto, Yukihiro},
  year      = {2008},
  publisher = {O'Reilly Media}
}

@article{
deng2020-resebm,
  doi = {10.48550/ARXIV.2004.11714},
  url = {https://arxiv.org/abs/2004.11714},
  author = {Deng, Yuntian and Bakhtin, Anton and Ott, Myle and Szlam, Arthur and Ranzato, Marc'Aurelio},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Residual Energy-Based Models for Text Generation},
  publisher = {arXiv},
  journal = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{gdb2022-bertmcmc,
  author    = {Kartik Goyal and
               Chris Dyer and
               Taylor Berg{-}Kirkpatrick},
  title     = {Exposing the Implicit Energy Networks behind Masked Language Models
               via Metropolis-Hastings},
  journal   = {CoRR},
  volume    = {abs/2106.02736},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.02736},
  eprinttype = {arXiv},
  eprint    = {2106.02736},
  timestamp = {Thu, 10 Jun 2021 16:34:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-02736.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{weng2021diffusion,
  title   = "What are diffusion models?",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2021",
  url     = "https://lilianweng.github.io/posts/2021-07-11-diffusion-models/"
}


@article{krotov2018dam,
  title={Dense Associative Memory Is Robust to Adversarial Inputs},
  author={Dmitry Krotov and John J. Hopfield},
  journal={Neural Computation},
  year={2018},
  volume={30},
  pages={3151-3167}
}



@article{krotov2016dam,
  author    = {Dmitry Krotov and
               John J. Hopfield},
  title     = {Dense Associative Memory for Pattern Recognition},
  journal   = {CoRR},
  volume    = {abs/1606.01164},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.01164},
  eprinttype = {arXiv},
  eprint    = {1606.01164},
  timestamp = {Mon, 13 Aug 2018 16:48:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KrotovH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{song-ermon2019ncse,
  author    = {Yang Song and
               Stefano Ermon},
  title     = {Generative Modeling by Estimating Gradients of the Data Distribution},
  journal   = {CoRR},
  volume    = {abs/1907.05600},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.05600},
  eprinttype = {arXiv},
  eprint    = {1907.05600},
  timestamp = {Thu, 01 Oct 2020 17:52:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-05600.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}









@article{
trRosetta-design-2021,
author = {Christoffer Norn  and Basile I. M. Wicky  and David Juergens  and Sirui Liu  and David Kim  and Doug Tischer  and Brian Koepnick  and Ivan Anishchenko  and null null  and David Baker  and Sergey Ovchinnikov  and Alan Coral  and Alex J. Bubar  and Alexander Boykov  and Alexander Uriel Valle Pérez  and Alison MacMillan  and Allen Lubow  and Andrea Mussini  and Andrew Cai  and Andrew John Ardill  and Aniruddha Seal  and Artak Kalantarian  and Barbara Failer  and Belinda Lackersteen  and Benjamin Chagot  and Beverly R. Haight  and Bora Taştan  and Boris Uitham  and Brandon G. Roy  and Breno Renan de Melo Cruz  and Brian Echols  and Brian Edward Lorenz  and Bruce Blair  and Bruno Kestemont  and C. D. Eastlake  and Callen Joseph Bragdon  and Carl Vardeman  and Carlo Salerno  and Casey Comisky  and Catherine Louise Hayman  and Catherine R Landers  and Cathy Zimov  and Charles David Coleman  and Charles Robert Painter  and Christopher Ince  and Conor Lynagh  and Dmitrii Malaniia  and Douglas Craig Wheeler  and Douglas Robertson  and Vera Simon  and Emanuele Chisari  and Eric Lim Jit Kai  and Farah Rezae  and Ferenc Lengyel  and Flavian Tabotta  and Franco Padelletti  and Frisno Boström  and Gary O. Gross  and George McIlvaine  and Gil Beecher  and Gregory T. Hansen  and Guido de Jong  and Harald Feldmann  and Jami Lynne Borman  and Jamie Quinn  and Jane Norrgard  and Jason Truong  and Jasper A. Diderich  and Jeffrey Michael Canfield  and Jeffrey Photakis  and Jesse David Slone  and Joanna Madzio  and Joanne Mitchell  and John Charles Stomieroski  and John H. Mitch  and Johnathan Robert Altenbeck  and Jonas Schinkler  and Jonathan Barak Weinberg  and Joshua David Burbach  and João Carlos Sequeira da Costa  and Juan Francisco Bada Juarez  and Jón Pétur Gunnarsson  and Kathleen Diane Harper  and Keehyoung Joo  and Keith T. Clayton  and Kenneth E. DeFord  and Kevin F. Scully  and Kevin M. Gildea  and Kirk J. Abbey  and Kristen Lee Kohli  and Kyle Stenner  and Kálmán Takács  and LaVerne L. Poussaint  and Larry C. Manalo  and Larry C. Withers  and Lilium Carlson  and Linda Wei  and Luke Ryan Fisher  and Lynn Carpenter  and Ma Ji-hwan  and Manuel Ricci  and Marcus Anthony Belcastro  and Marek Leniec  and Marie Hohmann  and Mark Thompson  and Matthew A. Thayer  and Matthias Gaebel  and Michael D. Cassidy  and Michael Fagiola  and Michael Lewis  and Michael Pfützenreuter  and Michael Simon  and Moamen M. Elmassry  and Noah Benevides  and Norah Kathleen Kerr  and Nupur Verma  and Oak Shannon  and Owen Yin  and Pascal Wolfteich  and Paul Gummersall  and Paweł Tłuścik  and Peter Gajar  and Peter John Triggiani  and Rajarshi Guha  and Renton Braden Mathew Innes  and Ricky Buchanan  and Robert Gamble  and Robert Leduc  and Robert Spearing  and Rodrigo Luccas Corrêa dos Santos Gomes  and Roger D. Estep  and Ryan DeWitt  and Ryan Moore  and Scott G. Shnider  and Scott J. Zaccanelli  and Sergey Kuznetsov  and Sergio Burillo-Sanz  and Seán Mooney  and Sidoruk Vasiliy  and Slava S. Butkovich  and Spencer Bruce Hudson  and Spencer Len Pote  and Stephen Phillip Denne  and Steven A. Schwegmann  and Sumanth Ratna  and Susan C. Kleinfelter  and Thomas Bausewein  and Thomas J. George  and Tobias Scherf de Almeida  and Ulas Yeginer  and Walter Barmettler  and Warwick Robert Pulley  and William Scott Wright  and null Willyanto  and Wyatt Lansford  and Xavier Hochart  and Yoan Anthony Skander Gaiji  and Yuriy Lagodich  and Vivier Christian },
title = {Protein sequence design by conformational landscape optimization},
journal = {Proceedings of the National Academy of Sciences},
volume = {118},
number = {11},
pages = {e2017228118},
year = {2021},
doi = {10.1073/pnas.2017228118},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2017228118},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2017228118},
abstract = {Almost all proteins fold to their lowest free energy state, which is determined by their amino acid sequence. Computational protein design has primarily focused on finding sequences that have very low energy in the target designed structure. However, what is most relevant during folding is not the absolute energy of the folded state but the energy difference between the folded state and the lowest-lying alternative states. We describe a deep learning approach that captures aspects of the folding landscape, in particular the presence of structures in alternative energy minima, and show that it can enhance current protein design methods. The protein design problem is to identify an amino acid sequence that folds to a desired structure. Given Anfinsen’s thermodynamic hypothesis of folding, this can be recast as finding an amino acid sequence for which the desired structure is the lowest energy state. As this calculation involves not only all possible amino acid sequences but also, all possible structures, most current approaches focus instead on the more tractable problem of finding the lowest-energy amino acid sequence for the desired structure, often checking by protein structure prediction in a second step that the desired structure is indeed the lowest-energy conformation for the designed sequence, and typically discarding a large fraction of designed sequences for which this is not the case. Here, we show that by backpropagating gradients through the transform-restrained Rosetta (trRosetta) structure prediction network from the desired structure to the input amino acid sequence, we can directly optimize over all possible amino acid sequences and all possible structures in a single calculation. We find that trRosetta calculations, which consider the full conformational landscape, can be more effective than Rosetta single-point energy estimations in predicting folding and stability of de novo designed proteins. We compare sequence design by conformational landscape optimization with the standard energy-based sequence design methodology in Rosetta and show that the former can result in energy landscapes with fewer alternative energy minima. We show further that more funneled energy landscapes can be designed by combining the strengths of the two approaches: the low-resolution trRosetta model serves to disfavor alternative states, and the high-resolution Rosetta model serves to create a deep energy minimum at the design target structure.}}


@article {protmpnn-dauparas2022,
	author = {Dauparas, J. and Anishchenko, I. and Bennett, N. and Bai, H. and Ragotte, R. J. and Milles, L. F. and Wicky, B. I. M. and Courbet, A. and de Haas, R. J. and Bethel, N. and Leung, P. J. Y. and Huddy, T. F. and Pellock, S. and Tischer, D. and Chan, F. and Koepnick, B. and Nguyen, H. and Kang, A. and Sankaran, B. and Bera, A. K. and King, N. P. and Baker, D.},
	title = {Robust deep learning based protein sequence design using ProteinMPNN},
	elocation-id = {2022.06.03.494563},
	year = {2022},
	doi = {10.1101/2022.06.03.494563},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {While deep learning has revolutionized protein structure prediction, almost all experimentally characterized de novo protein designs have been generated using physically based approaches such as Rosetta. Here we describe a deep learning based protein sequence design method, ProteinMPNN, with outstanding performance in both in silico and experimental tests. The amino acid sequence at different positions can be coupled between single or multiple chains, enabling application to a wide range of current protein design challenges. On native protein backbones, ProteinMPNN has a sequence recovery of 52.4\%, compared to 32.9\% for Rosetta. Incorporation of noise during training improves sequence recovery on protein structure models, and produces sequences which more robustly encode their structures as assessed using structure prediction algorithms. We demonstrate the broad utility and high accuracy of ProteinMPNN using X-ray crystallography, cryoEM and functional studies by rescuing previously failed designs, made using Rosetta or AlphaFold, of protein monomers, cyclic homo-oligomers, tetrahedral nanoparticles, and target binding proteins.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2022/06/04/2022.06.03.494563},
	eprint = {https://www.biorxiv.org/content/early/2022/06/04/2022.06.03.494563.full.pdf},
	journal = {bioRxiv}
}